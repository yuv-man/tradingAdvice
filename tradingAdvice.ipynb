{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e709fb3-b024-4420-a902-d38d01454850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded successfully\n",
      "Will analyze up to 10 stocks\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup environment and load libraries\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables if available\n",
    "try:\n",
    "    load_dotenv()\n",
    "    print(\"Environment variables loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e} - continuing without .env file\")\n",
    "\n",
    "# Create results directory\n",
    "output_dir = os.getenv('OUTPUT_DIR', './results')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get configuration from environment or set defaults\n",
    "MAX_SYMBOLS = int(os.getenv('MAX_SYMBOLS', '10'))\n",
    "print(f\"Will analyze up to {MAX_SYMBOLS} stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed7c14d-6003-4717-9c5c-c61b3ce86aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully loaded FinBERT model\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: FinBERT model setup for sentiment analysis\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# Set device (CPU or GPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} for inference\")\n",
    "\n",
    "def setup_finbert():\n",
    "    \"\"\"Setup FinBERT model for financial sentiment analysis\"\"\"\n",
    "    try:\n",
    "        # Load FinBERT model for financial sentiment analysis\n",
    "        model_name = \"ProsusAI/finbert\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        \n",
    "        # Move model to GPU if available\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Create sentiment analysis pipeline\n",
    "        nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0 if device == \"cuda\" else -1)\n",
    "        print(\"✓ Successfully loaded FinBERT model\")\n",
    "        return nlp\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FinBERT: {e}\")\n",
    "        print(\"Falling back to alternative model...\")\n",
    "        \n",
    "        try:\n",
    "            # Fallback to another financial model or general sentiment model\n",
    "            model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"  # General sentiment model\n",
    "            nlp = pipeline(\"sentiment-analysis\", model=model_name, device=0 if device == \"cuda\" else -1)\n",
    "            print(\"✓ Successfully loaded fallback sentiment model\")\n",
    "            return nlp\n",
    "        except Exception as e2:\n",
    "            print(f\"Error loading fallback model: {e2}\")\n",
    "            print(\"Unable to load any sentiment model. Will use rule-based sentiment.\")\n",
    "            return None\n",
    "\n",
    "# Initialize sentiment model\n",
    "sentiment_model = setup_finbert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2400f8d0-8fae-4965-a5db-5674fba1473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data fetching and preprocessing functions\n",
    "def validate_data_structure(data, symbol):\n",
    "    \"\"\"Validate and clean the data structure\"\"\"\n",
    "    if data is None or len(data) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Handle MultiIndex columns from yfinance\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        # Flatten columns by taking the first level\n",
    "        data.columns = [col[0] if isinstance(col, tuple) else col for col in data.columns]\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"Error: Missing columns {missing_columns} for {symbol}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to numeric and handle any potential issues\n",
    "    for col in required_columns:\n",
    "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "    \n",
    "    # Remove rows with any NaN values\n",
    "    initial_rows = len(data)\n",
    "    data = data.dropna()\n",
    "    if len(data) < initial_rows:\n",
    "        print(f\"Removed {initial_rows - len(data)} rows with NaN values for {symbol}\")\n",
    "    \n",
    "    # Check if we still have enough data\n",
    "    if len(data) < 50:\n",
    "        print(f\"Error: Insufficient data for {symbol}. Only {len(data)} rows available.\")\n",
    "        return None\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_sp500_symbols():\n",
    "    \"\"\"Fetch S&P 500 symbols from Wikipedia\"\"\"\n",
    "    print(\"Fetching S&P 500 symbols...\")\n",
    "    try:\n",
    "        url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table', {'class': 'wikitable'})\n",
    "        \n",
    "        symbols = []\n",
    "        for row in table.findAll('tr')[1:]:\n",
    "            symbol = row.findAll('td')[0].text.strip()\n",
    "            symbols.append(symbol)\n",
    "        \n",
    "        print(f\"Found {len(symbols)} S&P 500 symbols\")\n",
    "        return symbols\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching S&P 500 symbols: {e}\")\n",
    "        # Return a small fallback list of major companies\n",
    "        return ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'NVDA', 'JPM', 'JNJ', 'V']\n",
    "\n",
    "def get_historical_data(symbol, days=360):\n",
    "    \"\"\"Download historical stock data using yfinance\"\"\"\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=days)\n",
    "    \n",
    "    try:\n",
    "        # Download data\n",
    "        data = yf.download(symbol, start=start_date, end=end_date, progress=False)\n",
    "        \n",
    "        # Validate and clean the data structure\n",
    "        data = validate_data_structure(data, symbol)\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading data for {symbol}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ab3106e-ab1d-4188-8cc1-ef6948256c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Technical indicator calculation functions\n",
    "def ensure_series(data, column_name=None):\n",
    "    \"\"\"Ensure data is a pandas Series\"\"\"\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        if column_name and column_name in data.columns:\n",
    "            return data[column_name]\n",
    "        else:\n",
    "            return data.iloc[:, 0]  # Take first column\n",
    "    elif isinstance(data, pd.Series):\n",
    "        return data\n",
    "    else:\n",
    "        return pd.Series(data)\n",
    "\n",
    "def calculate_rsi(prices, period=14):\n",
    "    prices = ensure_series(prices)\n",
    "    delta = prices.diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    \n",
    "    avg_gain = gain.rolling(window=period).mean()\n",
    "    avg_loss = loss.rolling(window=period).mean()\n",
    "    \n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
    "    prices = ensure_series(prices)\n",
    "    ema_fast = prices.ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = prices.ewm(span=slow, adjust=False).mean()\n",
    "    macd = ema_fast - ema_slow\n",
    "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
    "    histogram = macd - signal_line\n",
    "    return macd, signal_line, histogram\n",
    "\n",
    "def calculate_atr(high, low, close, period=14):\n",
    "    high = ensure_series(high)\n",
    "    low = ensure_series(low)\n",
    "    close = ensure_series(close)\n",
    "    \n",
    "    tr1 = high - low\n",
    "    tr2 = abs(high - close.shift(1))\n",
    "    tr3 = abs(low - close.shift(1))\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    atr = tr.rolling(window=period).mean()\n",
    "    return atr\n",
    "\n",
    "def calculate_bollinger_bands(prices, period=20, std_dev=2):\n",
    "    prices = ensure_series(prices)\n",
    "    middle = prices.rolling(window=period).mean()\n",
    "    std = prices.rolling(window=period).std()\n",
    "    upper = middle + (std * std_dev)\n",
    "    lower = middle - (std * std_dev)\n",
    "    width = (upper - lower) / middle\n",
    "    return upper, middle, lower, width\n",
    "\n",
    "def calculate_obv(close, volume):\n",
    "    close = ensure_series(close)\n",
    "    volume = ensure_series(volume)\n",
    "    obv = pd.Series(0.0, index=close.index)\n",
    "    for i in range(1, len(close)):\n",
    "        if close.iloc[i] > close.iloc[i-1]:\n",
    "            obv.iloc[i] = obv.iloc[i-1] + volume.iloc[i]\n",
    "        elif close.iloc[i] < close.iloc[i-1]:\n",
    "            obv.iloc[i] = obv.iloc[i-1] - volume.iloc[i]\n",
    "        else:\n",
    "            obv.iloc[i] = obv.iloc[i-1]\n",
    "    return obv\n",
    "\n",
    "def calculate_stochastic(high, low, close, k_period=14, d_period=3):\n",
    "    high = ensure_series(high)\n",
    "    low = ensure_series(low)\n",
    "    close = ensure_series(close)\n",
    "    \n",
    "    high_roll = high.rolling(window=k_period).max()\n",
    "    low_roll = low.rolling(window=k_period).min()\n",
    "    stoch_k = 100 * (close - low_roll) / (high_roll - low_roll)\n",
    "    stoch_d = stoch_k.rolling(window=d_period).mean()\n",
    "    return stoch_k, stoch_d\n",
    "\n",
    "def calculate_adx(high, low, close, period=14):\n",
    "    high = ensure_series(high)\n",
    "    low = ensure_series(low)\n",
    "    close = ensure_series(close)\n",
    "    \n",
    "    plus_dm = high.diff()\n",
    "    minus_dm = low.diff()\n",
    "    plus_dm[plus_dm < 0] = 0\n",
    "    minus_dm[minus_dm > 0] = 0\n",
    "    \n",
    "    tr1 = high - low\n",
    "    tr2 = abs(high - close.shift(1))\n",
    "    tr3 = abs(low - close.shift(1))\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    \n",
    "    atr = tr.rolling(window=period).mean()\n",
    "    \n",
    "    plus_di = 100 * (plus_dm.rolling(window=period).mean() / atr)\n",
    "    minus_di = 100 * (abs(minus_dm.rolling(window=period).mean()) / atr)\n",
    "    \n",
    "    dx = 100 * (abs(plus_di - minus_di) / (plus_di + minus_di))\n",
    "    adx = dx.rolling(window=period).mean()\n",
    "    \n",
    "    return adx\n",
    "\n",
    "def calculate_flexible_technical_sentiment(df):\n",
    "    \"\"\"Calculate a sentiment score based on technical indicators\"\"\"\n",
    "    # Initialize sentiment series\n",
    "    sentiment = pd.Series(0.0, index=df.index)\n",
    "    \n",
    "    # Ensure all required columns exist and are Series\n",
    "    required_cols = ['RSI', 'MACD', 'Signal_Line', 'Close', 'SMA20', 'SMA50', 'Volume_Ratio', 'Daily_Return', 'BB_Lower', 'BB_Upper']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Warning: {col} not found in dataframe for sentiment calculation\")\n",
    "            return sentiment\n",
    "    \n",
    "    # RSI component (-0.4 to 0.4)\n",
    "    rsi_sentiment = pd.Series(0.0, index=df.index)\n",
    "    try:\n",
    "        rsi_sentiment[df['RSI'] < 30] = 0.4  # Oversold - positive sentiment\n",
    "        rsi_sentiment[(df['RSI'] >= 30) & (df['RSI'] <= 70)] = (df['RSI'] - 50) / 50 * 0.2  # Neutral zone\n",
    "        rsi_sentiment[df['RSI'] > 70] = -0.4  # Overbought - negative sentiment\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating RSI sentiment: {e}\")\n",
    "        rsi_sentiment = pd.Series(0.0, index=df.index)\n",
    "    \n",
    "    # MACD component (-0.2 to 0.2)\n",
    "    macd_sentiment = pd.Series(0.0, index=df.index)\n",
    "    try:\n",
    "        macd_sentiment[df['MACD'] > df['Signal_Line']] = 0.2  # Bullish\n",
    "        macd_sentiment[df['MACD'] <= df['Signal_Line']] = -0.2  # Bearish\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating MACD sentiment: {e}\")\n",
    "        macd_sentiment = pd.Series(0.0, index=df.index)\n",
    "    \n",
    "    # Moving Average component (-0.2 to 0.2)\n",
    "    ma_sentiment = pd.Series(0.0, index=df.index)\n",
    "    try:\n",
    "        ma_bullish_count = ((df['Close'] > df['SMA20']) & \n",
    "                            (df['Close'] > df['SMA50']) & \n",
    "                            (df['SMA20'] > df['SMA50'])).astype(int)\n",
    "        ma_bearish_count = ((df['Close'] < df['SMA20']) & \n",
    "                            (df['Close'] < df['SMA50']) & \n",
    "                            (df['SMA20'] < df['SMA50'])).astype(int)\n",
    "        ma_sentiment = (ma_bullish_count - ma_bearish_count) * 0.2\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating MA sentiment: {e}\")\n",
    "        ma_sentiment = pd.Series(0.0, index=df.index)\n",
    "    \n",
    "    # Bollinger Bands component (-0.1 to 0.1)\n",
    "    bb_sentiment = pd.Series(0.0, index=df.index)\n",
    "    try:\n",
    "        bb_sentiment[df['Close'] < df['BB_Lower']] = 0.1  # Below lower band - potential buy\n",
    "        bb_sentiment[df['Close'] > df['BB_Upper']] = -0.1  # Above upper band - potential sell\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BB sentiment: {e}\")\n",
    "        bb_sentiment = pd.Series(0.0, index=df.index)\n",
    "    \n",
    "    # Volume component (-0.1 to 0.1)\n",
    "    vol_sentiment = pd.Series(0.0, index=df.index)\n",
    "    try:\n",
    "        vol_sentiment[df['Volume_Ratio'] > 1.5] = 0.1 * np.sign(df['Daily_Return'])  # High volume in direction of move\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating volume sentiment: {e}\")\n",
    "        vol_sentiment = pd.Series(0.0, index=df.index)\n",
    "    \n",
    "    # Combine all components\n",
    "    sentiment = rsi_sentiment + macd_sentiment + ma_sentiment + bb_sentiment + vol_sentiment\n",
    "    \n",
    "    # Ensure sentiment is between -1 and 1\n",
    "    sentiment = sentiment.clip(-1, 1)\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "def get_technical_sentiment(data):\n",
    "    \"\"\"Calculate sentiment based on technical indicators\"\"\"\n",
    "    if data is None or len(data) < 5:\n",
    "        return 0.0\n",
    "    \n",
    "    try:\n",
    "        # Get the latest data point\n",
    "        latest = data.iloc[-1]\n",
    "        \n",
    "        # Initialize sentiment components\n",
    "        ma_sentiment = 0.0\n",
    "        momentum_sentiment = 0.0\n",
    "        volatility_sentiment = 0.0\n",
    "        \n",
    "        # Moving Average component (-0.4 to 0.4)\n",
    "        ma_signals = 0\n",
    "        ma_count = 0\n",
    "        \n",
    "        # Price vs moving averages\n",
    "        if 'SMA20' in latest.index and pd.notna(latest['SMA20']):\n",
    "            ma_signals += 1 if latest['Close'] > latest['SMA20'] else -1\n",
    "            ma_count += 1\n",
    "            \n",
    "        if 'SMA50' in latest.index and pd.notna(latest['SMA50']):\n",
    "            ma_signals += 1 if latest['Close'] > latest['SMA50'] else -1\n",
    "            ma_count += 1\n",
    "            \n",
    "        if 'SMA200' in latest.index and pd.notna(latest['SMA200']):\n",
    "            ma_signals += 1 if latest['Close'] > latest['SMA200'] else -1\n",
    "            ma_count += 1\n",
    "        \n",
    "        # Moving average crossovers\n",
    "        if 'SMA20' in latest.index and 'SMA50' in latest.index:\n",
    "            if pd.notna(latest['SMA20']) and pd.notna(latest['SMA50']):\n",
    "                ma_signals += 1 if latest['SMA20'] > latest['SMA50'] else -1\n",
    "                ma_count += 1\n",
    "        \n",
    "        if 'SMA50' in latest.index and 'SMA200' in latest.index:\n",
    "            if pd.notna(latest['SMA50']) and pd.notna(latest['SMA200']):\n",
    "                ma_signals += 1 if latest['SMA50'] > latest['SMA200'] else -1\n",
    "                ma_count += 1\n",
    "        \n",
    "        # Calculate moving average sentiment\n",
    "        if ma_count > 0:\n",
    "            ma_sentiment = (ma_signals / ma_count) * 0.4  # Scale to -0.4 to 0.4\n",
    "        \n",
    "        # Momentum indicators component (-0.4 to 0.4)\n",
    "        momentum_signals = 0\n",
    "        momentum_count = 0\n",
    "        \n",
    "        # RSI\n",
    "        if 'RSI' in latest.index and pd.notna(latest['RSI']):\n",
    "            rsi = latest['RSI']\n",
    "            if rsi < 30:\n",
    "                momentum_signals += 1  # Oversold - positive for future\n",
    "            elif rsi > 70:\n",
    "                momentum_signals -= 1  # Overbought - negative for future\n",
    "            momentum_count += 1\n",
    "        \n",
    "        # MACD\n",
    "        if 'MACD' in latest.index and 'Signal_Line' in latest.index:\n",
    "            if pd.notna(latest['MACD']) and pd.notna(latest['Signal_Line']):\n",
    "                momentum_signals += 1 if latest['MACD'] > latest['Signal_Line'] else -1\n",
    "                momentum_count += 1\n",
    "        \n",
    "        # ROC (Rate of Change)\n",
    "        if 'ROC10' in latest.index and pd.notna(latest['ROC10']):\n",
    "            roc = latest['ROC10']\n",
    "            if roc > 0:  # Rising price = positive momentum\n",
    "                momentum_signals += 1\n",
    "            else:  # Falling price = negative momentum\n",
    "                momentum_signals -= 1\n",
    "            momentum_count += 1\n",
    "        \n",
    "        # Calculate momentum sentiment\n",
    "        if momentum_count > 0:\n",
    "            momentum_sentiment = (momentum_signals / momentum_count) * 0.4  # Scale to -0.4 to 0.4\n",
    "        \n",
    "        # Volatility and Other Indicators component (-0.2 to 0.2)\n",
    "        vol_signals = 0\n",
    "        vol_count = 0\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        if all(item in latest.index for item in ['Close', 'BB_Upper', 'BB_Lower']):\n",
    "            if all(pd.notna(latest[item]) for item in ['Close', 'BB_Upper', 'BB_Lower']):\n",
    "                if latest['Close'] < latest['BB_Lower']:  # Oversold\n",
    "                    vol_signals += 1\n",
    "                elif latest['Close'] > latest['BB_Upper']:  # Overbought\n",
    "                    vol_signals -= 1\n",
    "                vol_count += 1\n",
    "        \n",
    "        # ATR (high volatility can be concerning)\n",
    "        if 'ATR' in latest.index and 'Close' in latest.index:\n",
    "            if pd.notna(latest['ATR']) and pd.notna(latest['Close']):\n",
    "                atr_pct = latest['ATR'] / latest['Close'] * 100\n",
    "                if atr_pct > 3:  # High volatility\n",
    "                    vol_signals -= 1\n",
    "                vol_count += 1\n",
    "        \n",
    "        # Volume indicators\n",
    "        if 'Volume_Ratio' in latest.index and pd.notna(latest['Volume_Ratio']):\n",
    "            volume_ratio = latest['Volume_Ratio']\n",
    "            if volume_ratio > 1.5:  # Higher than average volume\n",
    "                if 'Daily_Return' in latest.index and pd.notna(latest['Daily_Return']):\n",
    "                    # Volume in direction of price move\n",
    "                    vol_signals += 1 if latest['Daily_Return'] > 0 else -1\n",
    "                vol_count += 1\n",
    "        \n",
    "        # Calculate volatility sentiment\n",
    "        if vol_count > 0:\n",
    "            volatility_sentiment = (vol_signals / vol_count) * 0.2  # Scale to -0.2 to 0.2\n",
    "        \n",
    "        # Combine all components\n",
    "        total_sentiment = ma_sentiment + momentum_sentiment + volatility_sentiment\n",
    "        \n",
    "        # Ensure result is between -1 and 1\n",
    "        return max(-1.0, min(1.0, total_sentiment))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating technical sentiment: {e}\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84a031f6-ac12-41c2-8589-074de94ccad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Main feature engineering function (Fixed for smaller datasets)\n",
    "def calculate_technical_features(data):\n",
    "    \"\"\"Calculate comprehensive technical indicators with adaptive periods\"\"\"\n",
    "    if data is None or len(data) < 30:\n",
    "        print(f\"  Insufficient data: only {len(data) if data is not None else 0} rows\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = data.copy()\n",
    "        initial_length = len(df)\n",
    "        print(f\"  Starting with {initial_length} days of data\")\n",
    "        \n",
    "        # Ensure we have the right column structure\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]\n",
    "        \n",
    "        # Verify required columns exist\n",
    "        required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"  Error: Missing required columns: {missing_columns}\")\n",
    "            return None\n",
    "        \n",
    "        # Adaptive indicator calculation based on available data\n",
    "        data_length = len(df)\n",
    "        print(f\"  Calculating indicators for {data_length} days...\")\n",
    "        \n",
    "        # Always calculate these basic indicators\n",
    "        df['Daily_Return'] = df['Close'].pct_change()\n",
    "        df['Price_Range'] = (df['High'] - df['Low']) / df['Close'].replace(0, np.nan)\n",
    "        \n",
    "        # Short-term indicators (require less data)\n",
    "        if data_length >= 10:\n",
    "            df['SMA5'] = df['Close'].rolling(window=5).mean()\n",
    "            df['SMA10'] = df['Close'].rolling(window=10).mean()\n",
    "            df['EMA5'] = df['Close'].ewm(span=5, adjust=False).mean()\n",
    "            df['EMA10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
    "            df['ROC5'] = df['Close'].pct_change(periods=5) * 100\n",
    "            df['Volatility_5'] = df['Daily_Return'].rolling(window=5).std() * np.sqrt(252)\n",
    "        \n",
    "        # Medium-term indicators\n",
    "        if data_length >= 20:\n",
    "            df['SMA20'] = df['Close'].rolling(window=20).mean()\n",
    "            df['EMA20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
    "            df['ROC10'] = df['Close'].pct_change(periods=10) * 100\n",
    "            df['ROC20'] = df['Close'].pct_change(periods=20) * 100\n",
    "            \n",
    "            # RSI (needs at least 15 days)\n",
    "            df['RSI'] = calculate_rsi(df['Close'], period=14)\n",
    "            \n",
    "            # MACD\n",
    "            df['MACD'], df['Signal_Line'], df['MACD_Histogram'] = calculate_macd(df['Close'])\n",
    "            \n",
    "            # Bollinger Bands\n",
    "            df['BB_Upper'], df['BB_Middle'], df['BB_Lower'], df['BB_Width'] = calculate_bollinger_bands(df['Close'], period=20)\n",
    "            \n",
    "            # ATR\n",
    "            df['ATR'] = calculate_atr(df['High'], df['Low'], df['Close'], period=14)\n",
    "            \n",
    "            # Volume indicators\n",
    "            df['Volume_SMA20'] = df['Volume'].rolling(window=20).mean()\n",
    "            df['Volume_Ratio'] = df['Volume'] / df['Volume_SMA20'].replace(0, np.nan)\n",
    "            df['OBV'] = calculate_obv(df['Close'], df['Volume'])\n",
    "            \n",
    "            # Stochastic\n",
    "            df['K_percent'], df['D_percent'] = calculate_stochastic(df['High'], df['Low'], df['Close'])\n",
    "            \n",
    "            # Calculate ratios for short/medium term MAs\n",
    "            if 'SMA10' in df.columns:\n",
    "                df['Close_SMA10_Ratio'] = df['Close'] / df['SMA10'].replace(0, np.nan)\n",
    "            if 'SMA20' in df.columns:\n",
    "                df['Close_SMA20_Ratio'] = df['Close'] / df['SMA20'].replace(0, np.nan)\n",
    "        \n",
    "        # Longer-term indicators (only if we have enough data)\n",
    "        if data_length >= 50:\n",
    "            df['SMA50'] = df['Close'].rolling(window=50).mean()\n",
    "            df['EMA50'] = df['Close'].ewm(span=50, adjust=False).mean()\n",
    "            df['Close_SMA50_Ratio'] = df['Close'] / df['SMA50'].replace(0, np.nan)\n",
    "            df['ROC50'] = df['Close'].pct_change(periods=50) * 100\n",
    "            df['Volatility_30'] = df['Daily_Return'].rolling(window=30).std() * np.sqrt(252)\n",
    "            \n",
    "            # ADX (needs more data)\n",
    "            df['ADX'] = calculate_adx(df['High'], df['Low'], df['Close'], period=14)\n",
    "        \n",
    "        # Very long-term indicators (only with lots of data)\n",
    "        if data_length >= 100:\n",
    "            df['SMA100'] = df['Close'].rolling(window=100).mean()\n",
    "            df['EMA100'] = df['Close'].ewm(span=100, adjust=False).mean()\n",
    "            df['Close_SMA100_Ratio'] = df['Close'] / df['SMA100'].replace(0, np.nan)\n",
    "        \n",
    "        if data_length >= 220:  # Only with 200+ extra buffer\n",
    "            df['SMA200'] = df['Close'].rolling(window=200).mean()\n",
    "            df['EMA200'] = df['Close'].ewm(span=200, adjust=False).mean()\n",
    "            df['Close_SMA200_Ratio'] = df['Close'] / df['SMA200'].replace(0, np.nan)\n",
    "        \n",
    "        # Create target variables\n",
    "        df['Next_Close'] = df['Close'].shift(-1)\n",
    "        df['Target'] = ((df['Next_Close'] > df['Close']) * 1).astype(int)\n",
    "        df['Target_Return'] = ((df['Next_Close'] - df['Close']) / df['Close'].replace(0, np.nan)) * 100\n",
    "        \n",
    "        # Create lagged variables (only for available features)\n",
    "        lag_features = ['Close', 'Volume']\n",
    "        if 'RSI' in df.columns:\n",
    "            lag_features.append('RSI')\n",
    "        if 'MACD' in df.columns:\n",
    "            lag_features.append('MACD')\n",
    "            \n",
    "        for feature in lag_features:\n",
    "            if feature in df.columns:\n",
    "                df[f'{feature}_Lag1'] = df[feature].shift(1)\n",
    "                df[f'{feature}_Lag2'] = df[feature].shift(2)\n",
    "        \n",
    "        # Calculate technical sentiment based on available indicators\n",
    "        df['Tech_Sentiment'] = calculate_flexible_technical_sentiment(df)\n",
    "        \n",
    "        # SMART NaN removal - be more aggressive about keeping data\n",
    "        print(f\"  Before NaN removal: {len(df)} rows\")\n",
    "        \n",
    "        # Drop rows where the target is NaN (last row)\n",
    "        df = df[df['Target'].notna()]\n",
    "        \n",
    "        # For other NaN values, be more selective\n",
    "        # Only require the most basic indicators to be non-NaN\n",
    "        essential_features = ['Close', 'Daily_Return']\n",
    "        if 'RSI' in df.columns:\n",
    "            essential_features.append('RSI')\n",
    "        if 'SMA20' in df.columns:\n",
    "            essential_features.append('SMA20')\n",
    "        \n",
    "        # Drop rows where essential features are NaN\n",
    "        for feature in essential_features:\n",
    "            if feature in df.columns:\n",
    "                initial_rows = len(df)\n",
    "                df = df[df[feature].notna()]\n",
    "                dropped = initial_rows - len(df)\n",
    "                if dropped > 0:\n",
    "                    print(f\"    Dropped {dropped} rows due to NaN in {feature}\")\n",
    "        \n",
    "        # Fill remaining NaN values with forward-fill then back-fill\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_columns] = df[numeric_columns].fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        print(f\"  After processing: {len(df)} rows with {len(df.columns)} features\")\n",
    "        \n",
    "        # Final check - ensure we have enough data for meaningful analysis\n",
    "        if len(df) < 20:\n",
    "            print(f\"  Warning: Very little data remaining ({len(df)} rows). Consider using more historical data.\")\n",
    "            if len(df) < 10:\n",
    "                print(f\"  Insufficient data for analysis.\")\n",
    "                return None\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error in calculate_technical_features: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fa41c71-f28c-42b5-b2de-2010420b7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t re\n",
    "                        pct_match = re.search(r'([-+]?)([0-9.]+)%', text)\n",
    "                        if pct_match:\n",
    "                            sign = pct_match.group(1)\n",
    "                            value = float(pct_match.group(2))\n",
    "                            pct_change = value if sign != '-' else -value\n",
    "                            \n",
    "                            # Convert percentage change to sentiment\n",
    "                            # +5% or more = positive, -5% or less = negative\n",
    "                            if pct_change > 5:\n",
    "                                sentiment = 0.5\n",
    "                            elif pct_change > 2:\n",
    "                                sentiment = 0.25\n",
    "                            elif pct_change < -5:\n",
    "                                sentiment = -0.5\n",
    "                            elif pct_change < -2:\n",
    "                                sentiment = -0.25\n",
    "                            else:\n",
    "                                sentiment = 0.0\n",
    "                            \n",
    "                            print(f\"    Estimated analyst sentiment from price change ({pct_change:.1f}%): {sentiment:.2f}\")\n",
    "                            return sentiment\n",
    "                    except (ValueError, AttributeError):\n",
    "                        continue\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"    Fallback analyst method failed: {e}\")\n",
    "    \n",
    "    print(\"    All analyst sentiment methods failed, returning neutral\")\n",
    "    return 0.0  # Neutral if not found\n",
    "\n",
    "def analyze_news_sentiment_rule_based(news_data):\n",
    "    \"\"\"Simple rule-based sentiment analysis as fallback\"\"\"\n",
    "    if not news_data:\n",
    "        return 0.0\n",
    "    \n",
    "    # Define positive and negative word lists for financial context\n",
    "    positive_words = [\n",
    "        'gain', 'gains', 'up', 'rise', 'rises', 'rising', 'rose', 'bullish', 'outperform',\n",
    "        'buy', 'growth', 'profit', 'profits', 'positive', 'strong', 'strength', 'higher',\n",
    "        'record', 'upgrade', 'upgraded', 'beat', 'beats', 'exceed', 'exceeds', 'success',\n",
    "        'successful', 'increase', 'increases', 'increased', 'boost', 'boosts', 'boosted',\n",
    "        'opportunity', 'opportunities', 'potential', 'promising', 'optimistic', 'confident',\n",
    "        'momentum', 'surge', 'rally', 'advance', 'advances', 'breakthrough', 'expansion',\n",
    "        'outperforming', 'soars', 'jumped', 'climbed', 'accelerate', 'accelerating'\n",
    "    ]\n",
    "    \n",
    "    negative_words = [\n",
    "        'loss', 'losses', 'down', 'fall', 'falls', 'falling', 'fell', 'bearish', 'underperform',\n",
    "        'sell', 'decline', 'declines', 'declined', 'negative', 'weak', 'weakness', 'lower',\n",
    "        'downgrade', 'downgraded', 'miss', 'misses', 'missed', 'fail', 'fails', 'failed',\n",
    "        'decrease', 'decreases', 'decreased', 'cut', 'cuts', 'risk', 'risks', 'risky',\n",
    "        'concern', 'concerns', 'warning', 'problem', 'problems', 'threat', 'threats',\n",
    "        'disappointing', 'crash', 'plunge', 'plummeted', 'volatile', 'uncertainty',\n",
    "        'recession', 'bear', 'correction', 'selloff', 'slump', 'struggle', 'struggling'\n",
    "    ]\n",
    "    \n",
    "    # Intensifier words that modify sentiment\n",
    "    intensifiers = {\n",
    "        'very': 1.5, 'extremely': 2.0, 'significantly': 1.7, 'substantially': 1.7,\n",
    "        'dramatically': 2.0, 'sharply': 1.8, 'strongly': 1.6, 'heavily': 1.5,\n",
    "        'massive': 2.0, 'huge': 1.8, 'major': 1.5, 'significant': 1.3\n",
    "    }\n",
    "    \n",
    "    total_sentiment = 0\n",
    "    article_count = 0\n",
    "    \n",
    "    for article in news_data:\n",
    "        text = (article['headline'] + \" \" + article.get('summary', '')).lower()\n",
    "        words = text.split()\n",
    "        \n",
    "        # Count positive and negative words\n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            # Clean the word\n",
    "            clean_word = word.strip('.,!?;:\"()[]')\n",
    "            \n",
    "            # Check for intensifiers\n",
    "            intensifier = 1.0\n",
    "            if i > 0:\n",
    "                prev_word = words[i-1].strip('.,!?;:\"()[]')\n",
    "                if prev_word in intensifiers:\n",
    "                    intensifier = intensifiers[prev_word]\n",
    "            \n",
    "            # Count sentiment words with intensifier\n",
    "            if clean_word in positive_words:\n",
    "                positive_count += intensifier\n",
    "            elif clean_word in negative_words:\n",
    "                negative_count += intensifier\n",
    "        \n",
    "        # Calculate sentiment for this article\n",
    "        if positive_count > 0 or negative_count > 0:\n",
    "            # Normalize by total sentiment words found\n",
    "            total_sentiment_words = positive_count + negative_count\n",
    "            article_sentiment = (positive_count - negative_count) / total_sentiment_words\n",
    "            \n",
    "            # Scale to reasonable range\n",
    "            article_sentiment = max(-1.0, min(1.0, article_sentiment))\n",
    "            total_sentiment += article_sentiment\n",
    "            article_count += 1\n",
    "    \n",
    "    # Average sentiment across all articles\n",
    "    if article_count > 0:\n",
    "        average_sentiment = total_sentiment / article_count\n",
    "        return average_sentiment\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def analyze_news_sentiment_with_finbert(news_data, nlp):\n",
    "    \"\"\"Analyze sentiment using FinBERT with enhanced fallbacks\"\"\"\n",
    "    if not news_data:\n",
    "        return 0.0  # Neutral sentiment if no news\n",
    "    \n",
    "    # If FinBERT model is not available, use rule-based approach\n",
    "    if nlp is None:\n",
    "        print(\"    Using rule-based sentiment analysis (FinBERT not available)\")\n",
    "        return analyze_news_sentiment_rule_based(news_data)\n",
    "    \n",
    "    sentiments = []\n",
    "    \n",
    "    for article in news_data:\n",
    "        text = article['headline']\n",
    "        if len(article.get('summary', '')) > 0:\n",
    "            text += \" \" + article['summary']\n",
    "        \n",
    "        # Skip empty text\n",
    "        if not text.strip():\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Try FinBERT analysis\n",
    "            result = nlp(text[:500])  # Limit text length\n",
    "            \n",
    "            if result and len(result) > 0:\n",
    "                # Map FinBERT sentiment labels to scores\n",
    "                sent_label = result[0]['label'].lower()\n",
    "                sent_score = result[0]['score']\n",
    "                \n",
    "                if 'positive' in sent_label:\n",
    "                    sentiments.append(sent_score)\n",
    "                elif 'negative' in sent_label:\n",
    "                    sentiments.append(-sent_score)\n",
    "                else:  # neutral\n",
    "                    sentiments.append(0.0)\n",
    "            else:\n",
    "                # Fallback to rule-based for this article\n",
    "                rule_sentiment = analyze_news_sentiment_rule_based([article])\n",
    "                sentiments.append(rule_sentiment)\n",
    "                \n",
    "        except Exception as e:\n",
    "            # If FinBERT fails, use rule-based for this article\n",
    "            try:\n",
    "                rule_sentiment = analyze_news_sentiment_rule_based([article])\n",
    "                sentiments.append(rule_sentiment)\n",
    "            except:\n",
    "                sentiments.append(0.0)\n",
    "    \n",
    "    # Average all sentiment scores\n",
    "    if sentiments:\n",
    "        avg_sentiment = sum(sentiments) / len(sentiments)\n",
    "        # Scale to ensure [-1, 1] range\n",
    "        return max(-1.0, min(1.0, avg_sentiment))\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# Update the main news function\n",
    "def get_stock_news(symbol, max_articles=5):\n",
    "    \"\"\"Get recent news articles about a stock symbol\"\"\"\n",
    "    return get_stock_news_robust(symbol, max_articles)\n",
    "\n",
    "def get_sentiment_analysis(symbol, data=None):\n",
    "    \"\"\"Get comprehensive sentiment analysis for a stock using FinBERT\"\"\"\n",
    "    print(f\"  Analyzing sentiment for {symbol}...\")\n",
    "    \n",
    "    # Initialize sentiment components\n",
    "    news_sentiment = 0.0\n",
    "    analyst_sentiment = 0.0\n",
    "    tech_sentiment = 0.0\n",
    "    \n",
    "    # Get news sentiment using FinBERT\n",
    "    try:\n",
    "        news_data = get_stock_news(symbol)\n",
    "        news_sentiment = analyze_news_sentiment_with_finbert(news_data, sentiment_model)\n",
    "        print(f\"    News sentiment: {news_sentiment:.2f} (from {len(news_data)} articles)\")\n",
    "    except Exception as e:\n",
    "        print(f\"    News sentiment failed: {e}\")\n",
    "        news_sentiment = 0.0\n",
    "    \n",
    "    # Get analyst ratings\n",
    "    try:\n",
    "        analyst_sentiment = get_analyst_ratings(symbol)\n",
    "        print(f\"    Analyst sentiment: {analyst_sentiment:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Analyst sentiment failed: {e}\")\n",
    "        analyst_sentiment = 0.0\n",
    "    \n",
    "    # Get technical sentiment\n",
    "    try:\n",
    "        if data is not None:\n",
    "            tech_sentiment = get_technical_sentiment(data)\n",
    "            print(f\"    Technical sentiment: {tech_sentiment:.2f}\")\n",
    "        else:\n",
    "            tech_sentiment = 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"    Technical sentiment failed: {e}\")\n",
    "        tech_sentiment = 0.0\n",
    "    \n",
    "    # Weight the components with fallbacks\n",
    "    # If we have all three: 40% news, 30% analyst, 30% technical\n",
    "    # If missing news: 50% analyst, 50% technical\n",
    "    # If missing analyst: 60% news, 40% technical\n",
    "    # If only technical: 100% technical\n",
    "    \n",
    "    components = []\n",
    "    weights = []\n",
    "    \n",
    "    if abs(news_sentiment) > 0.001:  # We have meaningful news sentiment\n",
    "        components.append(news_sentiment)\n",
    "        weights.append(0.4)\n",
    "    \n",
    "    if abs(analyst_sentiment) > 0.001:  # We have meaningful analyst sentiment\n",
    "        components.append(analyst_sentiment)\n",
    "        weights.append(0.3)\n",
    "    \n",
    "    if abs(tech_sentiment) > 0.001:  # We have meaningful technical sentiment\n",
    "        components.append(tech_sentiment)\n",
    "        weights.append(0.3)\n",
    "    \n",
    "    # Normalize weights\n",
    "    if weights:\n",
    "        total_weight = sum(weights)\n",
    "        weights = [w/total_weight for w in weights]\n",
    "        combined_sentiment = sum(c*w for c, w in zip(components, weights))\n",
    "    else:\n",
    "        # Fallback to neutral sentiment\n",
    "        combined_sentiment = 0.0\n",
    "    \n",
    "    print(f\"    Combined sentiment: {combined_sentiment:.2f}\")\n",
    "    \n",
    "    # Add a small random variation to avoid all stocks having identical sentiment scores\n",
    "    final_sentiment = combined_sentiment + (random.uniform(-0.05, 0.05))\n",
    "    final_sentiment = max(-1.0, min(1.0, final_sentiment))\n",
    "    \n",
    "    # Add a small delay to avoid hitting rate limits\n",
    "    time.sleep(random.uniform(1, 2))\n",
    "    \n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9e7f467-5b42-432f-8b2e-2b2e37355ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Enhanced ML Model Training Functions (Fixed for small datasets)\n",
    "def create_ensemble_models_adaptive(data_size):\n",
    "    \"\"\"Create models adapted to the size of available data\"\"\"\n",
    "    if data_size < 50:\n",
    "        # Very small dataset - use simple models\n",
    "        models = {\n",
    "            'logistic_regression': LogisticRegression(\n",
    "                random_state=42,\n",
    "                max_iter=100,\n",
    "                solver='liblinear',\n",
    "                C=0.1  # More regularization\n",
    "            )\n",
    "        }\n",
    "        print(f\"    Using simple model for small dataset ({data_size} samples)\")\n",
    "    elif data_size < 100:\n",
    "        # Small dataset - use regularized models\n",
    "        models = {\n",
    "            'random_forest': RandomForestClassifier(\n",
    "                n_estimators=20,\n",
    "                max_depth=5,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=5,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'logistic_regression': LogisticRegression(\n",
    "                random_state=42,\n",
    "                max_iter=200,\n",
    "                solver='liblinear',\n",
    "                C=0.1\n",
    "            )\n",
    "        }\n",
    "        print(f\"    Using regularized models for medium dataset ({data_size} samples)\")\n",
    "    else:\n",
    "        # Larger dataset - use full ensemble\n",
    "        models = {\n",
    "            'random_forest': RandomForestClassifier(\n",
    "                n_estimators=50,\n",
    "                max_depth=10,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'gradient_boosting': GradientBoostingClassifier(\n",
    "                n_estimators=50,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'logistic_regression': LogisticRegression(\n",
    "                random_state=42,\n",
    "                max_iter=500,\n",
    "                solver='liblinear',\n",
    "                C=1.0\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Add XGBoost for larger datasets\n",
    "        try:\n",
    "            models['xgboost'] = xgb.XGBClassifier(\n",
    "                n_estimators=50,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                random_state=42,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='logloss',\n",
    "                verbosity=0\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"    XGBoost not available: {e}\")\n",
    "        \n",
    "        print(f\"    Using full ensemble for large dataset ({data_size} samples)\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def prepare_ml_data_robust(data):\n",
    "    \"\"\"Prepare and clean data for ML training with enhanced small dataset handling\"\"\"\n",
    "    try:\n",
    "        print(f\"  Preparing ML data from {len(data)} rows...\")\n",
    "        \n",
    "        # Select features for ML, excluding target and non-predictive columns\n",
    "        exclude_cols = [\n",
    "            'Target', 'Target_Return', 'Next_Close', \n",
    "            'Tech_Sentiment',  # Used separately as External_Sentiment\n",
    "        ]\n",
    "        \n",
    "        # Get all potential feature columns\n",
    "        all_feature_cols = [col for col in data.columns if col not in exclude_cols]\n",
    "        \n",
    "        # Keep only numeric columns\n",
    "        numeric_cols = []\n",
    "        for col in all_feature_cols:\n",
    "            if pd.api.types.is_numeric_dtype(data[col]):\n",
    "                numeric_cols.append(col)\n",
    "        \n",
    "        print(f\"    Found {len(numeric_cols)} numeric features\")\n",
    "        \n",
    "        # Create feature matrix and target\n",
    "        X = data[numeric_cols].copy()\n",
    "        y = data['Target'].copy()\n",
    "        \n",
    "        # Handle remaining NaN values more aggressively\n",
    "        print(f\"    Handling missing values...\")\n",
    "        \n",
    "        # Count NaN values per column\n",
    "        nan_counts = X.isna().sum()\n",
    "        \n",
    "        # Remove columns with too many NaN values (>50% missing)\n",
    "        threshold = len(X) * 0.5\n",
    "        good_cols = []\n",
    "        for col in X.columns:\n",
    "            if nan_counts[col] <= threshold:\n",
    "                good_cols.append(col)\n",
    "            else:\n",
    "                print(f\"      Removing {col}: {nan_counts[col]}/{len(X)} missing values\")\n",
    "        \n",
    "        X = X[good_cols]\n",
    "        \n",
    "        # Fill remaining NaN values with median\n",
    "        for col in X.columns:\n",
    "            if X[col].isna().any():\n",
    "                median_val = X[col].median()\n",
    "                if pd.isna(median_val):  # If median is also NaN, use 0\n",
    "                    median_val = 0\n",
    "                n_filled = X[col].isna().sum()\n",
    "                X[col].fillna(median_val, inplace=True)\n",
    "                print(f\"      Filled {n_filled} NaN values in {col} with {median_val:.3f}\")\n",
    "        \n",
    "        # Handle infinite values\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "        X = X.fillna(X.median())\n",
    "        X = X.fillna(0)  # Final fallback\n",
    "        \n",
    "        # Remove features with zero variance\n",
    "        from sklearn.feature_selection import VarianceThreshold\n",
    "        variance_selector = VarianceThreshold(threshold=0.0)\n",
    "        X_filtered = variance_selector.fit_transform(X)\n",
    "        selected_features = X.columns[variance_selector.get_support()].tolist()\n",
    "        X = pd.DataFrame(X_filtered, columns=selected_features, index=X.index)\n",
    "        \n",
    "        if len(selected_features) < len(numeric_cols):\n",
    "            removed = len(numeric_cols) - len(selected_features)\n",
    "            print(f\"      Removed {removed} zero-variance features\")\n",
    "        \n",
    "        # Ensure we have valid target values\n",
    "        valid_mask = y.notna()\n",
    "        X = X[valid_mask]\n",
    "        y = y[valid_mask]\n",
    "        \n",
    "        # Final data checks\n",
    "        print(f\"    Final dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "        \n",
    "        # Check target distribution\n",
    "        target_counts = y.value_counts()\n",
    "        print(f\"    Target distribution: {target_counts.to_dict()}\")\n",
    "        \n",
    "        return X, y, selected_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    Error preparing ML data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "def train_and_predict_robust(data):\n",
    "    \"\"\"Enhanced ML training with small dataset support\"\"\"\n",
    "    if data is None:\n",
    "        print(\"  No data provided for ML training\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Starting ML training with {len(data)} data points...\")\n",
    "    \n",
    "    # Lower the minimum data requirement\n",
    "    if len(data) < 20:\n",
    "        print(f\"  Insufficient data for ML training (need at least 20, got {len(data)})\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Prepare the data\n",
    "        X, y, feature_cols = prepare_ml_data_robust(data)\n",
    "        \n",
    "        if X is None or len(X) < 15:\n",
    "            print(\"  Failed to prepare sufficient data for ML\")\n",
    "            return None\n",
    "        \n",
    "        # Check class balance\n",
    "        class_counts = y.value_counts()\n",
    "        if len(class_counts) < 2:\n",
    "            print(f\"  Only one class present: {class_counts.to_dict()}\")\n",
    "            return None\n",
    "        \n",
    "        min_class_count = class_counts.min()\n",
    "        if min_class_count < 3:\n",
    "            print(f\"  Insufficient class balance for ML: {class_counts.to_dict()}\")\n",
    "            print(\"  (Need at least 3 samples of each class)\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"    Class balance OK: {class_counts.to_dict()}\")\n",
    "        \n",
    "        # Use adaptive test size based on total data\n",
    "        if len(X) < 30:\n",
    "            test_size = 0.2  # Keep more for training\n",
    "        elif len(X) < 60:\n",
    "            test_size = 0.25\n",
    "        else:\n",
    "            test_size = 0.3\n",
    "        \n",
    "        # Ensure minimum test size\n",
    "        min_test_samples = max(3, min_class_count // 2)\n",
    "        actual_test_size = max(test_size, min_test_samples / len(X))\n",
    "        \n",
    "        print(f\"    Using test size: {actual_test_size:.2f}\")\n",
    "        \n",
    "        # Split data\n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, \n",
    "                test_size=actual_test_size, \n",
    "                stratify=y, \n",
    "                random_state=42\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            print(f\"    Stratified split failed ({e}), using random split\")\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, \n",
    "                test_size=actual_test_size, \n",
    "                random_state=42\n",
    "            )\n",
    "        \n",
    "        print(f\"    Train: {len(X_train)} samples, Test: {len(X_test)} samples\")\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Get adaptive models\n",
    "        models = create_ensemble_models_adaptive(len(X_train))\n",
    "        model_results = {}\n",
    "        successful_models = 0\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            try:\n",
    "                print(f\"      Training {name}...\")\n",
    "                \n",
    "                # Train the model\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                \n",
    "                # Make predictions\n",
    "                if len(X_test) > 0:\n",
    "                    test_pred = model.predict(X_test_scaled)\n",
    "                    test_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "                    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "                else:\n",
    "                    # If no test set, use train set for evaluation (not ideal but better than failure)\n",
    "                    test_pred = model.predict(X_train_scaled)\n",
    "                    test_proba = model.predict_proba(X_train_scaled)[:, 1]\n",
    "                    test_accuracy = accuracy_score(y_train, test_pred)\n",
    "                    print(f\"        Warning: Using training set for evaluation\")\n",
    "                \n",
    "                # Train accuracy\n",
    "                train_pred = model.predict(X_train_scaled)\n",
    "                train_accuracy = accuracy_score(y_train, train_pred)\n",
    "                \n",
    "                # Check for overfitting\n",
    "                overfitting = train_accuracy - test_accuracy\n",
    "                \n",
    "                model_results[name] = {\n",
    "                    'model': model,\n",
    "                    'train_accuracy': train_accuracy,\n",
    "                    'test_accuracy': test_accuracy,\n",
    "                    'prediction': test_pred[-1] if len(test_pred) > 0 else train_pred[-1],\n",
    "                    'probability': test_proba[-1] if len(test_proba) > 0 else 0.5,\n",
    "                    'overfitting': overfitting\n",
    "                }\n",
    "                \n",
    "                successful_models += 1\n",
    "                print(f\"        ✓ Train={train_accuracy:.3f}, Test={test_accuracy:.3f}\")\n",
    "                \n",
    "                if overfitting > 0.3:\n",
    "                    print(f\"        ⚠ High overfitting: {overfitting:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"        ❌ {name} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if successful_models == 0:\n",
    "            print(\"  All ML models failed to train\")\n",
    "            return None\n",
    "        \n",
    "        # Ensemble prediction\n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "        weights = []\n",
    "        \n",
    "        for name, result in model_results.items():\n",
    "            # Weight by test accuracy, penalize overfitting\n",
    "            weight = result['test_accuracy'] * (1 - min(result['overfitting'], 0.5))\n",
    "            weight = max(weight, 0.1)  # Minimum weight\n",
    "            weights.append(weight)\n",
    "            predictions.append(result['prediction'])\n",
    "            probabilities.append(result['probability'])\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        weights = np.array(weights)\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        ensemble_prediction = np.average(predictions, weights=weights)\n",
    "        ensemble_probability = np.average(probabilities, weights=weights)\n",
    "        average_accuracy = np.average([r['test_accuracy'] for r in model_results.values()])\n",
    "        \n",
    "        # Get feature importance from best model\n",
    "        best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['test_accuracy'])\n",
    "        best_model = model_results[best_model_name]['model']\n",
    "        \n",
    "        importance_dict = {}\n",
    "        if hasattr(best_model, 'feature_importances_'):\n",
    "            importance_dict[best_model_name] = dict(zip(feature_cols, best_model.feature_importances_))\n",
    "        elif hasattr(best_model, 'coef_'):\n",
    "            importance_dict[best_model_name] = dict(zip(feature_cols, np.abs(best_model.coef_[0])))\n",
    "        \n",
    "        result = {\n",
    "            'prediction': ensemble_prediction,\n",
    "            'probability': ensemble_probability,\n",
    "            'accuracy': average_accuracy,\n",
    "            'model_results': model_results,\n",
    "            'feature_importance': importance_dict,\n",
    "            'successful_models': successful_models,\n",
    "            'best_model': best_model_name,\n",
    "            'data_size': len(X)\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✓ ML training complete: {successful_models} models, accuracy={average_accuracy:.3f}\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ML training error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Update the main training function\n",
    "def train_and_predict(data):\n",
    "    \"\"\"Main ML training function with enhanced robustness for small datasets\"\"\"\n",
    "    return train_and_predict_robust(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0346b899-7732-47da-b2a4-76bbe9600723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Generate stock recommendations (Fixed to be more responsive)\n",
    "def generate_enhanced_recommendation(technical_data, sentiment_score, ml_result):\n",
    "    \"\"\"Generate trading recommendations based on technical, sentiment, and ML data\"\"\"\n",
    "    if technical_data is None or len(technical_data) < 5:\n",
    "        return \"INSUFFICIENT_DATA\"\n",
    "    \n",
    "    latest = technical_data.iloc[-1]\n",
    "    \n",
    "    # Technical analysis score (more detailed)\n",
    "    technical_score = 0\n",
    "    \n",
    "    # Moving Average signals (stronger weight for clear trends)\n",
    "    ma_signals = 0\n",
    "    ma_count = 0\n",
    "    \n",
    "    # Check available moving averages\n",
    "    if 'SMA20' in latest.index and pd.notna(latest['SMA20']):\n",
    "        if latest['Close'] > latest['SMA20']:\n",
    "            ma_signals += 1\n",
    "        else:\n",
    "            ma_signals -= 1\n",
    "        ma_count += 1\n",
    "    \n",
    "    if 'SMA50' in latest.index and pd.notna(latest['SMA50']):\n",
    "        if latest['Close'] > latest['SMA50']:\n",
    "            ma_signals += 1.5  # More weight for longer MA\n",
    "        else:\n",
    "            ma_signals -= 1.5\n",
    "        ma_count += 1\n",
    "    \n",
    "    if 'SMA200' in latest.index and pd.notna(latest['SMA200']):\n",
    "        if latest['Close'] > latest['SMA200']:\n",
    "            ma_signals += 2  # Strong weight for long-term trend\n",
    "        else:\n",
    "            ma_signals -= 2\n",
    "        ma_count += 1\n",
    "    \n",
    "    # Check MA crossovers\n",
    "    if 'SMA20' in latest.index and 'SMA50' in latest.index:\n",
    "        if pd.notna(latest['SMA20']) and pd.notna(latest['SMA50']):\n",
    "            if latest['SMA20'] > latest['SMA50']:\n",
    "                ma_signals += 1\n",
    "            else:\n",
    "                ma_signals -= 1\n",
    "            ma_count += 1\n",
    "    \n",
    "    # Normalize MA signals\n",
    "    if ma_count > 0:\n",
    "        technical_score += ma_signals / ma_count * 3  # Scale up the MA influence\n",
    "    \n",
    "    # Momentum signals (RSI)\n",
    "    if 'RSI' in latest.index and pd.notna(latest['RSI']):\n",
    "        rsi = latest['RSI']\n",
    "        if rsi < 25:  # Very oversold - strong buy signal\n",
    "            technical_score += 3\n",
    "        elif rsi < 35:  # Oversold - buy signal\n",
    "            technical_score += 2\n",
    "        elif rsi > 75:  # Very overbought - strong sell signal\n",
    "            technical_score -= 3\n",
    "        elif rsi > 65:  # Overbought - sell signal\n",
    "            technical_score -= 2\n",
    "        # Neutral zone (35-65) adds no score\n",
    "    \n",
    "    # MACD signals\n",
    "    if 'MACD' in latest.index and 'Signal_Line' in latest.index:\n",
    "        if pd.notna(latest['MACD']) and pd.notna(latest['Signal_Line']):\n",
    "            if latest['MACD'] > latest['Signal_Line']:\n",
    "                technical_score += 2\n",
    "            else:\n",
    "                technical_score -= 2\n",
    "    \n",
    "    # Trend strength (ADX)\n",
    "    if 'ADX' in latest.index and pd.notna(latest['ADX']):\n",
    "        adx = latest['ADX']\n",
    "        if adx > 40:  # Strong trend\n",
    "            # Determine trend direction from price vs MAs\n",
    "            if 'SMA20' in latest.index and pd.notna(latest['SMA20']):\n",
    "                if latest['Close'] > latest['SMA20']:\n",
    "                    technical_score += 1  # Strong uptrend\n",
    "                else:\n",
    "                    technical_score -= 1  # Strong downtrend\n",
    "    \n",
    "    # Price momentum (recent performance)\n",
    "    if 'ROC10' in latest.index and pd.notna(latest['ROC10']):\n",
    "        roc = latest['ROC10']\n",
    "        if roc > 5:  # Strong positive momentum\n",
    "            technical_score += 2\n",
    "        elif roc > 2:  # Positive momentum\n",
    "            technical_score += 1\n",
    "        elif roc < -5:  # Strong negative momentum\n",
    "            technical_score -= 2\n",
    "        elif roc < -2:  # Negative momentum\n",
    "            technical_score -= 1\n",
    "    \n",
    "    # Normalize technical score (-10 to +10 range)\n",
    "    technical_score = max(-10, min(10, technical_score))\n",
    "    normalized_technical = technical_score / 10.0  # Scale to -1 to 1\n",
    "    \n",
    "    print(f\"    Technical score: {technical_score}/10 ({normalized_technical:.2f})\")\n",
    "    \n",
    "    # ML prediction weight\n",
    "    ml_weight = 0\n",
    "    ml_confidence = 0.5\n",
    "    \n",
    "    if ml_result:\n",
    "        # Scale ML probability to -1 to 1 range\n",
    "        ml_weight = (ml_result['probability'] - 0.5) * 2\n",
    "        ml_confidence = ml_result.get('accuracy', 0.5)\n",
    "        print(f\"    ML weight: {ml_weight:.2f} (prob: {ml_result['probability']:.2f}, accuracy: {ml_confidence:.2f})\")\n",
    "    \n",
    "    # Sentiment weight (already in -1 to 1 range)\n",
    "    print(f\"    Sentiment: {sentiment_score:.2f}\")\n",
    "    \n",
    "    # Combine all scores with weights\n",
    "    # Increased weights to make system more responsive\n",
    "    if ml_confidence >= 0.6:\n",
    "        # High ML confidence: 35% technical, 45% ML, 20% sentiment\n",
    "        final_score = (normalized_technical * 0.35) + (ml_weight * 0.45) + (sentiment_score * 0.20)\n",
    "    else:\n",
    "        # Low ML confidence: 50% technical, 25% ML, 25% sentiment\n",
    "        final_score = (normalized_technical * 0.50) + (ml_weight * 0.25) + (sentiment_score * 0.25)\n",
    "    \n",
    "    print(f\"    Final combined score: {final_score:.3f}\")\n",
    "    \n",
    "    # Generate recommendation with more aggressive thresholds\n",
    "    if ml_confidence < 0.5:\n",
    "        # Very low confidence - be very conservative\n",
    "        if final_score > 0.6:\n",
    "            return \"WEAK_BUY\"\n",
    "        elif final_score < -0.6:\n",
    "            return \"WEAK_SELL\"\n",
    "        else:\n",
    "            return \"HOLD\"\n",
    "    elif ml_confidence < 0.65:\n",
    "        # Medium confidence - somewhat conservative\n",
    "        if final_score > 0.4:\n",
    "            return \"BUY\"\n",
    "        elif final_score > 0.15:\n",
    "            return \"WEAK_BUY\"\n",
    "        elif final_score < -0.4:\n",
    "            return \"SELL\"\n",
    "        elif final_score < -0.15:\n",
    "            return \"WEAK_SELL\"\n",
    "        else:\n",
    "            return \"HOLD\"\n",
    "    else:\n",
    "        # High confidence - more aggressive recommendations\n",
    "        if final_score > 0.25:\n",
    "            return \"STRONG_BUY\"\n",
    "        elif final_score > 0.1:\n",
    "            return \"BUY\"\n",
    "        elif final_score > 0.05:\n",
    "            return \"WEAK_BUY\"\n",
    "        elif final_score < -0.25:\n",
    "            return \"STRONG_SELL\"\n",
    "        elif final_score < -0.1:\n",
    "            return \"SELL\"\n",
    "        elif final_score < -0.05:\n",
    "            return \"WEAK_SELL\"\n",
    "        else:\n",
    "            return \"HOLD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e21126da-6fe1-406e-9b58-185fbecffd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Cell 9: Main analysis function (with better debugging)\n",
    "def analyze_stock(symbol, show_details=True):\n",
    "    \"\"\"Analyze a single stock with detailed output using FinBERT sentiment analysis\"\"\"\n",
    "    print(f\"Analyzing {symbol}...\")\n",
    "    \n",
    "    # Get historical data\n",
    "    historical_data = get_historical_data(symbol)\n",
    "    if historical_data is None or len(historical_data) < 100:\n",
    "        print(f\"❌ Insufficient historical data for {symbol}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate technical features\n",
    "    technical_data = calculate_technical_features(historical_data)\n",
    "    if technical_data is None:\n",
    "        print(f\"❌ Failed to calculate technical indicators for {symbol}\")\n",
    "        return None\n",
    "    \n",
    "    # Get sentiment analysis using FinBERT\n",
    "    sentiment_score = get_sentiment_analysis(symbol, technical_data)\n",
    "    print(f\"✓ Sentiment score: {sentiment_score:.2f}\")\n",
    "    \n",
    "    # Add sentiment to the data for ML\n",
    "    technical_data['External_Sentiment'] = sentiment_score\n",
    "    \n",
    "    # Train ML models and predict\n",
    "    ml_result = train_and_predict(technical_data)\n",
    "    if ml_result:\n",
    "        print(f\"✓ ML model accuracy: {ml_result['accuracy']:.2f}\")\n",
    "        print(f\"✓ ML prediction probability: {ml_result['probability']:.2f}\")\n",
    "    else:\n",
    "        print(\"❌ ML prediction failed\")\n",
    "    \n",
    "    # Generate recommendation with detailed scoring\n",
    "    print(\"  Generating recommendation:\")\n",
    "    recommendation = generate_enhanced_recommendation(technical_data, sentiment_score, ml_result)\n",
    "    print(f\"✓ Final recommendation: {recommendation}\")\n",
    "    \n",
    "    # Create result summary\n",
    "    result = {\n",
    "        'Symbol': symbol,\n",
    "        'Last_Price': technical_data['Close'].iloc[-1],\n",
    "        'RSI': technical_data['RSI'].iloc[-1] if 'RSI' in technical_data.columns else None,\n",
    "        'SMA20': technical_data['SMA20'].iloc[-1] if 'SMA20' in technical_data.columns else None,\n",
    "        'SMA50': technical_data['SMA50'].iloc[-1] if 'SMA50' in technical_data.columns else None,\n",
    "        'SMA200': technical_data['SMA200'].iloc[-1] if 'SMA200' in technical_data.columns else None,\n",
    "        'MACD': technical_data['MACD'].iloc[-1] if 'MACD' in technical_data.columns else None,\n",
    "        'ADX': technical_data['ADX'].iloc[-1] if 'ADX' in technical_data.columns else None,\n",
    "        'Sentiment': sentiment_score,\n",
    "        'Recommendation': recommendation\n",
    "    }\n",
    "    \n",
    "    # Add ML results if available\n",
    "    if ml_result:\n",
    "        result['ML_Prediction'] = ml_result['prediction']\n",
    "        result['ML_Probability'] = ml_result['probability']\n",
    "        result['ML_Accuracy'] = ml_result['accuracy']\n",
    "    \n",
    "    # Show additional details if requested\n",
    "    if show_details:\n",
    "        print(f\"\\n  Detailed Analysis for {symbol}:\")\n",
    "        print(f\"  Current Price: ${result['Last_Price']:.2f}\")\n",
    "        if result['RSI']:\n",
    "            print(f\"  RSI: {result['RSI']:.1f}\")\n",
    "        if result['SMA20']:\n",
    "            print(f\"  Price vs SMA20: {((result['Last_Price']/result['SMA20']-1)*100):+.1f}%\")\n",
    "        if result['SMA50']:\n",
    "            print(f\"  Price vs SMA50: {((result['Last_Price']/result['SMA50']-1)*100):+.1f}%\")\n",
    "        \n",
    "        # Show feature importance if available\n",
    "        if ml_result and 'feature_importance' in ml_result and show_details:\n",
    "            best_model = ml_result.get('best_model', 'random_forest')\n",
    "            if best_model in ml_result['feature_importance']:\n",
    "                importances = ml_result['feature_importance'][best_model]\n",
    "                top_features = dict(sorted(importances.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "                print(f\"  Top 5 features ({best_model}):\")\n",
    "                for feature, importance in top_features.items():\n",
    "                    print(f\"    {feature}: {importance:.3f}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1be8ee40-1129-4076-917f-6da40bf0fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Process multiple stocks\n",
    "def analyze_multiple_stocks(symbols, max_stocks=None):\n",
    "    \"\"\"Analyze multiple stocks and return a DataFrame of results\"\"\"\n",
    "    if max_stocks:\n",
    "        symbols = symbols[:max_stocks]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, symbol in enumerate(symbols):\n",
    "        print(f\"Analyzing {symbol} ({i+1}/{len(symbols)})\")\n",
    "        \n",
    "        try:\n",
    "            result = analyze_stock(symbol, show_details=False)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {symbol}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Rate limiting to avoid being blocked by web services\n",
    "        if (i + 1) % 3 == 0 and i < len(symbols) - 1:\n",
    "            sleep_time = 10 + random.randint(1, 5)\n",
    "            print(f\"Pausing for {sleep_time} seconds to avoid rate limiting...\")\n",
    "            time.sleep(sleep_time)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebcd3608-7bfd-49ea-8dcb-1cdfddb9dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Visualization and results with robust column handling\n",
    "def plot_results(results_df):\n",
    "    \"\"\"Create visualizations and summary of results with better error handling\"\"\"\n",
    "    # Display recommendations summary\n",
    "    print(\"\\nRecommendation Summary:\")\n",
    "    rec_counts = results_df['Recommendation'].value_counts()\n",
    "    display(rec_counts)\n",
    "\n",
    "    # Plot recommendation distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    rec_order = [\n",
    "        'STRONG_BUY', 'BUY', 'WEAK_BUY', \n",
    "        'HOLD', \n",
    "        'WEAK_SELL', 'SELL', 'STRONG_SELL'\n",
    "    ]\n",
    "    # Only include categories that exist in the data\n",
    "    existing_categories = [cat for cat in rec_order if cat in rec_counts.index]\n",
    "    \n",
    "    if existing_categories:\n",
    "        sns.countplot(x='Recommendation', data=results_df, order=existing_categories)\n",
    "        plt.title('Stock Recommendations Distribution')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No recommendation data to plot\")\n",
    "\n",
    "    # Ensure we have required columns for displays\n",
    "    required_cols = ['Symbol', 'Last_Price', 'Recommendation']\n",
    "    missing_cols = [col for col in required_cols if col not in results_df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"Missing required columns for display: {missing_cols}\")\n",
    "        return results_df, results_df  # Return empty DataFrames as placeholders\n",
    "        \n",
    "    # Optional columns to display if available\n",
    "    display_cols = ['Symbol', 'Last_Price', 'Recommendation']\n",
    "    for col in ['RSI', 'ADX', 'Sentiment', 'ML_Probability', 'ML_Accuracy']:\n",
    "        if col in results_df.columns:\n",
    "            display_cols.append(col)\n",
    "    \n",
    "    # Show top buy recommendations\n",
    "    buy_stocks = results_df[results_df['Recommendation'].isin(['STRONG_BUY', 'BUY'])].sort_values('Last_Price', ascending=True)\n",
    "    if 'ML_Probability' in results_df.columns:\n",
    "        buy_stocks = results_df[results_df['Recommendation'].isin(['STRONG_BUY', 'BUY'])].sort_values('ML_Probability', ascending=False)\n",
    "    \n",
    "    print(\"\\nTOP BUY RECOMMENDATIONS:\")\n",
    "    if len(buy_stocks) > 0:\n",
    "        display(buy_stocks[display_cols])\n",
    "    else:\n",
    "        print(\"No buy recommendations found\")\n",
    "\n",
    "    # Show top sell recommendations\n",
    "    sell_stocks = results_df[results_df['Recommendation'].isin(['STRONG_SELL', 'SELL'])].sort_values('Last_Price', ascending=True)\n",
    "    if 'ML_Probability' in results_df.columns:\n",
    "        sell_stocks = results_df[results_df['Recommendation'].isin(['STRONG_SELL', 'SELL'])].sort_values('ML_Probability', ascending=True)\n",
    "    \n",
    "    print(\"\\nTOP SELL RECOMMENDATIONS:\")\n",
    "    if len(sell_stocks) > 0:\n",
    "        display(sell_stocks[display_cols])\n",
    "    else:\n",
    "        print(\"No sell recommendations found\")\n",
    "\n",
    "    # Only create scatter plot if we have the necessary columns\n",
    "    if 'Sentiment' in results_df.columns and 'ML_Probability' in results_df.columns:\n",
    "        try:\n",
    "            # Create scatter plot of Sentiment vs ML_Probability colored by Recommendation\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            recommendation_colors = {\n",
    "                'STRONG_BUY': 'darkgreen',\n",
    "                'BUY': 'green',\n",
    "                'WEAK_BUY': 'lightgreen',\n",
    "                'HOLD': 'blue',\n",
    "                'WEAK_SELL': 'salmon',\n",
    "                'SELL': 'red',\n",
    "                'STRONG_SELL': 'darkred'\n",
    "            }\n",
    "\n",
    "            for rec in recommendation_colors:\n",
    "                subset = results_df[results_df['Recommendation'] == rec]\n",
    "                if len(subset) > 0:\n",
    "                    plt.scatter(subset['Sentiment'], subset['ML_Probability'], \n",
    "                              c=recommendation_colors[rec], label=rec, s=100, alpha=0.7)\n",
    "\n",
    "            for i, row in results_df.iterrows():\n",
    "                plt.annotate(row['Symbol'], \n",
    "                            (row['Sentiment'], row['ML_Probability']),\n",
    "                            xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "            plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.3)\n",
    "            plt.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "            plt.xlabel('Sentiment Score')\n",
    "            plt.ylabel('ML Probability (Higher = More Bullish)')\n",
    "            plt.title('Stock Analysis: Sentiment vs ML Prediction')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            plt.savefig(os.path.join(output_dir, f\"sentiment_ml_plot_{timestamp}.png\"))\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating scatter plot: {e}\")\n",
    "    else:\n",
    "        print(\"Cannot create scatter plot: missing required columns (Sentiment and/or ML_Probability)\")\n",
    "    \n",
    "    return buy_stocks, sell_stocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1168bf8-4cc8-45e4-8b6d-19c3af71d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Save results and create a styled dataframe\n",
    "def save_and_display_results(results_df):\n",
    "    \"\"\"Save results to files and create styled display with better error handling\"\"\"\n",
    "    if len(results_df) == 0:\n",
    "        print(\"No results to save or display\")\n",
    "        return results_df.style  # Return empty styled DataFrame\n",
    "        \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = os.getenv('OUTPUT_DIR', './results')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save results to CSV\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_file = os.path.join(output_dir, f\"sp500_analysis_{timestamp}.csv\")\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "    \n",
    "    # Create HTML report (with error handling for missing columns)\n",
    "    try:\n",
    "        # Count recommendations\n",
    "        rec_counts = {\n",
    "            'STRONG_BUY': len(results_df[results_df['Recommendation'] == 'STRONG_BUY']),\n",
    "            'BUY': len(results_df[results_df['Recommendation'] == 'BUY']), \n",
    "            'HOLD': len(results_df[results_df['Recommendation'] == 'HOLD']),\n",
    "            'SELL': len(results_df[results_df['Recommendation'] == 'SELL']),\n",
    "            'STRONG_SELL': len(results_df[results_df['Recommendation'] == 'STRONG_SELL'])\n",
    "        }\n",
    "        \n",
    "        # Get buy and sell recommendations\n",
    "        buy_recs = results_df[results_df['Recommendation'].isin(['STRONG_BUY', 'BUY'])]\n",
    "        sell_recs = results_df[results_df['Recommendation'].isin(['STRONG_SELL', 'SELL'])]\n",
    "        \n",
    "        html_report = f\"\"\"\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>S&P 500 Analysis Report - {datetime.now().strftime('%Y-%m-%d')}</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
    "                table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}\n",
    "                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: center; }}\n",
    "                th {{ background-color: #4CAF50; color: white; }}\n",
    "                .buy {{ color: green; font-weight: bold; }}\n",
    "                .sell {{ color: red; font-weight: bold; }}\n",
    "                .hold {{ color: blue; font-weight: bold; }}\n",
    "                .summary {{ margin: 20px 0; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>S&P 500 Analysis Report</h1>\n",
    "            <p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "            \n",
    "            <div class=\"summary\">\n",
    "                <h2>Summary</h2>\n",
    "                <p>Total Stocks Analyzed: {len(results_df)}</p>\n",
    "                <ul>\n",
    "                    <li>Strong Buy: {rec_counts['STRONG_BUY']}</li>\n",
    "                    <li>Buy: {rec_counts['BUY']}</li>\n",
    "                    <li>Hold: {rec_counts['HOLD']}</li>\n",
    "                    <li>Sell: {rec_counts['SELL']}</li>\n",
    "                    <li>Strong Sell: {rec_counts['STRONG_SELL']}</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "            \n",
    "            <h2>Top Buy Recommendations</h2>\n",
    "            {buy_recs.to_html(classes='data', index=False) if len(buy_recs) > 0 else \"<p>No buy recommendations found.</p>\"}\n",
    "            \n",
    "            <h2>Top Sell Recommendations</h2>\n",
    "            {sell_recs.to_html(classes='data', index=False) if len(sell_recs) > 0 else \"<p>No sell recommendations found.</p>\"}\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        html_file = os.path.join(output_dir, f\"analysis_report_{timestamp}.html\")\n",
    "        with open(html_file, 'w') as f:\n",
    "            f.write(html_report)\n",
    "        \n",
    "        print(f\"HTML report saved to: {html_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating HTML report: {e}\")\n",
    "    \n",
    "    # Style DataFrame for display\n",
    "    def color_recommendation(val):\n",
    "        if val == 'STRONG_BUY':\n",
    "            return 'background-color: darkgreen; color: white'\n",
    "        elif val == 'BUY':\n",
    "            return 'background-color: green; color: white'\n",
    "        elif val == 'WEAK_BUY':\n",
    "            return 'background-color: lightgreen'\n",
    "        elif val == 'HOLD':\n",
    "            return 'background-color: lightblue'\n",
    "        elif val == 'WEAK_SELL':\n",
    "            return 'background-color: salmon'\n",
    "        elif val == 'SELL':\n",
    "            return 'background-color: red; color: white'\n",
    "        elif val == 'STRONG_SELL':\n",
    "            return 'background-color: darkred; color: white'\n",
    "        return ''\n",
    "\n",
    "    # Return styled DataFrame\n",
    "    try:\n",
    "        return results_df.style.applymap(color_recommendation, subset=['Recommendation'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error styling DataFrame: {e}\")\n",
    "        return results_df  # Return unstyles DataFrame as fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd91c9f4-dafd-408f-9478-1b037c46804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Interactive analysis of a single stock (optional)\n",
    "def interactive_single_stock_analysis(symbol=\"AAPL\"):\n",
    "    \"\"\"Interactive analysis of a single stock with detailed output\"\"\"\n",
    "    result = analyze_stock(symbol, show_details=True)\n",
    "    if result:\n",
    "        # Plot recent price history\n",
    "        historical_data = get_historical_data(symbol, days=360)\n",
    "        if historical_data is not None:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(historical_data.index, historical_data['Close'])\n",
    "            plt.title(f\"{symbol} Price History - Last 360 Days\")\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"Price ($)\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Technical indicators plot\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 12), sharex=True)\n",
    "            \n",
    "            # Price and moving averages\n",
    "            ax1.plot(historical_data.index, historical_data['Close'], label='Close')\n",
    "            ax1.plot(historical_data.index, historical_data['Close'].rolling(window=20).mean(), label='SMA20')\n",
    "            ax1.plot(historical_data.index, historical_data['Close'].rolling(window=50).mean(), label='SMA50')\n",
    "            ax1.plot(historical_data.index, historical_data['Close'].rolling(window=200).mean(), label='SMA200')\n",
    "            ax1.set_title(f\"{symbol} Price and Moving Averages\")\n",
    "            ax1.set_ylabel(\"Price ($)\")\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # RSI\n",
    "            rsi = calculate_rsi(historical_data['Close'])\n",
    "            ax2.plot(historical_data.index, rsi, color='purple')\n",
    "            ax2.axhline(y=70, color='r', linestyle='-', alpha=0.3)\n",
    "            ax2.axhline(y=30, color='g', linestyle='-', alpha=0.3)\n",
    "            ax2.set_title(\"RSI (14)\")\n",
    "            ax2.set_ylabel(\"RSI\")\n",
    "            ax2.set_ylim(0, 100)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # MACD\n",
    "            macd, signal, hist = calculate_macd(historical_data['Close'])\n",
    "            ax3.plot(historical_data.index, macd, label='MACD')\n",
    "            ax3.plot(historical_data.index, signal, label='Signal')\n",
    "            ax3.bar(historical_data.index, hist, label='Histogram', alpha=0.5)\n",
    "            ax3.set_title(\"MACD\")\n",
    "            ax3.set_ylabel(\"MACD\")\n",
    "            ax3.set_xlabel(\"Date\")\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Show final recommendation with styled color\n",
    "            recommendation = result['Recommendation']\n",
    "            rec_color = {\n",
    "                'STRONG_BUY': '🟢 <span style=\"color:darkgreen;font-weight:bold;\">STRONG BUY</span>',\n",
    "                'BUY': '🟢 <span style=\"color:green;font-weight:bold;\">BUY</span>',\n",
    "                'WEAK_BUY': '🟢 <span style=\"color:lightgreen;font-weight:bold;\">WEAK BUY</span>',\n",
    "                'HOLD': '🔵 <span style=\"color:blue;font-weight:bold;\">HOLD</span>',\n",
    "                'WEAK_SELL': '🔴 <span style=\"color:salmon;font-weight:bold;\">WEAK SELL</span>',\n",
    "                'SELL': '🔴 <span style=\"color:red;font-weight:bold;\">SELL</span>',\n",
    "                'STRONG_SELL': '🔴 <span style=\"color:darkred;font-weight:bold;\">STRONG SELL</span>'\n",
    "            }\n",
    "            \n",
    "            display(HTML(f\"<h2>Final recommendation for {symbol}: {rec_color.get(recommendation, recommendation)}</h2>\"))\n",
    "            \n",
    "            return result\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2be4c94a-f9a5-4047-a613-81ac7e25e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Testing and validation functions\n",
    "def test_single_stock_analysis(symbol=\"AAPL\"):\n",
    "    \"\"\"Test the analysis pipeline with a single stock\"\"\"\n",
    "    print(f\"Testing analysis pipeline with {symbol}...\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Download data\n",
    "        print(\"1. Downloading historical data...\")\n",
    "        historical_data = get_historical_data(symbol, days=360)\n",
    "        if historical_data is None:\n",
    "            print(\"❌ Failed to download data\")\n",
    "            return False\n",
    "        print(f\"✓ Downloaded {len(historical_data)} days of data\")\n",
    "        print(f\"  Columns: {list(historical_data.columns)}\")\n",
    "        print(f\"  Data types: {historical_data.dtypes.to_dict()}\")\n",
    "        \n",
    "        # Step 2: Calculate technical features\n",
    "        print(\"2. Calculating technical features...\")\n",
    "        technical_data = calculate_technical_features(historical_data)\n",
    "        if technical_data is None:\n",
    "            print(\"❌ Failed to calculate technical features\")\n",
    "            return False\n",
    "        print(f\"✓ Calculated {len(technical_data.columns)} technical features\")\n",
    "        print(f\"  Final dataset shape: {technical_data.shape}\")\n",
    "        \n",
    "        # Step 3: Test sentiment analysis\n",
    "        print(\"3. Testing sentiment analysis...\")\n",
    "        try:\n",
    "            sentiment_score = get_sentiment_analysis(symbol, technical_data)\n",
    "            print(f\"✓ Sentiment analysis completed: {sentiment_score:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Sentiment analysis failed: {e}\")\n",
    "            sentiment_score = 0.0\n",
    "        \n",
    "        # Step 4: Test ML prediction\n",
    "        print(\"4. Testing ML prediction...\")\n",
    "        technical_data['External_Sentiment'] = sentiment_score\n",
    "        ml_result = train_and_predict(technical_data)\n",
    "        if ml_result:\n",
    "            print(f\"✓ ML prediction successful\")\n",
    "            print(f\"  Ensemble accuracy: {ml_result['accuracy']:.3f}\")\n",
    "            print(f\"  Prediction probability: {ml_result['probability']:.3f}\")\n",
    "        else:\n",
    "            print(\"⚠ ML prediction failed\")\n",
    "        \n",
    "        # Step 5: Generate recommendation\n",
    "        print(\"5. Generating recommendation...\")\n",
    "        recommendation = generate_enhanced_recommendation(technical_data, sentiment_score, ml_result)\n",
    "        print(f\"✓ Final recommendation: {recommendation}\")\n",
    "        \n",
    "        print(f\"\\n✅ Test completed successfully for {symbol}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cc11905-a1f1-4d13-a926-1ec9fc889f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Quick analysis function with error handling\n",
    "def quick_analyze_stock(symbol, show_details=False):\n",
    "    \"\"\"Quick analysis with comprehensive error handling\"\"\"\n",
    "    result = {\n",
    "        'Symbol': symbol,\n",
    "        'Status': 'Failed',\n",
    "        'Error': None,\n",
    "        'Last_Price': None,\n",
    "        'RSI': None,\n",
    "        'MACD': None,\n",
    "        'Sentiment': None,\n",
    "        'ML_Probability': None,\n",
    "        'Recommendation': 'INSUFFICIENT_DATA'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get data with timeout and retry logic\n",
    "        print(f\"Analyzing {symbol}...\")\n",
    "        \n",
    "        for attempt in range(2):  # Try twice\n",
    "            historical_data = get_historical_data(symbol)\n",
    "            if historical_data is not None:\n",
    "                break\n",
    "            if attempt == 0:\n",
    "                print(f\"  Retry attempt for {symbol}...\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        if historical_data is None:\n",
    "            result['Error'] = 'Failed to download data'\n",
    "            return result\n",
    "        \n",
    "        # Calculate features with error handling\n",
    "        try:\n",
    "            technical_data = calculate_technical_features(historical_data)\n",
    "            if technical_data is None:\n",
    "                result['Error'] = 'Failed to calculate technical features'\n",
    "                return result\n",
    "        except Exception as e:\n",
    "            result['Error'] = f'Technical analysis error: {str(e)}'\n",
    "            return result\n",
    "        \n",
    "        # Extract basic metrics\n",
    "        latest = technical_data.iloc[-1]\n",
    "        result['Last_Price'] = latest['Close']\n",
    "        result['RSI'] = latest.get('RSI', None)\n",
    "        result['MACD'] = latest.get('MACD', None)\n",
    "        \n",
    "        # Get sentiment with fallback\n",
    "        try:\n",
    "            sentiment_score = get_sentiment_analysis(symbol, technical_data)\n",
    "            result['Sentiment'] = sentiment_score\n",
    "        except Exception as e:\n",
    "            print(f\"  Sentiment analysis failed, using technical sentiment: {e}\")\n",
    "            result['Sentiment'] = get_technical_sentiment(technical_data)\n",
    "        \n",
    "        # ML prediction with fallback\n",
    "        technical_data['External_Sentiment'] = result['Sentiment']\n",
    "        try:\n",
    "            ml_result = train_and_predict(technical_data)\n",
    "            if ml_result:\n",
    "                result['ML_Probability'] = ml_result['probability']\n",
    "                result['ML_Accuracy'] = ml_result['accuracy']\n",
    "            else:\n",
    "                # Simple prediction based on technical indicators\n",
    "                result['ML_Probability'] = 0.5 + (result['Sentiment'] * 0.3)\n",
    "        except Exception as e:\n",
    "            print(f\"  ML prediction failed, using simple heuristic: {e}\")\n",
    "            result['ML_Probability'] = 0.5 + (result['Sentiment'] * 0.3)\n",
    "        \n",
    "        # Generate recommendation\n",
    "        try:\n",
    "            recommendation = generate_enhanced_recommendation(\n",
    "                technical_data, \n",
    "                result['Sentiment'], \n",
    "                ml_result if 'ml_result' in locals() else None\n",
    "            )\n",
    "            result['Recommendation'] = recommendation\n",
    "        except Exception as e:\n",
    "            # Simple recommendation based on sentiment and basic indicators\n",
    "            if result['Sentiment'] > 0.3 and result['RSI'] < 70:\n",
    "                result['Recommendation'] = 'BUY'\n",
    "            elif result['Sentiment'] < -0.3 and result['RSI'] > 30:\n",
    "                result['Recommendation'] = 'SELL'\n",
    "            else:\n",
    "                result['Recommendation'] = 'HOLD'\n",
    "        \n",
    "        result['Status'] = 'Success'\n",
    "        print(f\"✓ {symbol}: {result['Recommendation']} (Sentiment: {result['Sentiment']:.2f})\")\n",
    "        \n",
    "        if show_details:\n",
    "            print(f\"  Price: ${result['Last_Price']:.2f}\")\n",
    "            print(f\"  RSI: {result['RSI']:.1f}\" if result['RSI'] else \"  RSI: N/A\")\n",
    "            print(f\"  ML Prob: {result['ML_Probability']:.3f}\" if result['ML_Probability'] else \"  ML Prob: N/A\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['Error'] = str(e)\n",
    "        print(f\"❌ {symbol}: Analysis failed - {e}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73d35240-1dd2-40bb-8e42-f60477f012a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Running initial test...\n",
      "Testing analysis pipeline with BTC...\n",
      "1. Downloading historical data...\n",
      "✓ Downloaded 202 days of data\n",
      "  Columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
      "  Data types: {'Close': dtype('float64'), 'High': dtype('float64'), 'Low': dtype('float64'), 'Open': dtype('float64'), 'Volume': dtype('int64')}\n",
      "2. Calculating technical features...\n",
      "  Starting with 202 days of data\n",
      "  Calculating indicators for 202 days...\n",
      "  Before NaN removal: 202 rows\n",
      "    Dropped 1 rows due to NaN in Daily_Return\n",
      "    Dropped 12 rows due to NaN in RSI\n",
      "    Dropped 6 rows due to NaN in SMA20\n",
      "  After processing: 183 rows with 54 features\n",
      "✓ Calculated 54 technical features\n",
      "  Final dataset shape: (183, 54)\n",
      "3. Testing sentiment analysis...\n",
      "  Analyzing sentiment for BTC...\n",
      "    Trying yahoo_finance...\n",
      "    ⚠ No articles from yahoo_finance\n",
      "    Trying finviz...\n",
      "    ✓ Found 5 articles from finviz\n",
      "    News sentiment: 0.00 (from 5 articles)\n",
      "    Estimated analyst sentiment from price change (-0.8%): 0.00\n",
      "    Analyst sentiment: 0.00\n",
      "    Technical sentiment: 0.53\n",
      "    Combined sentiment: 0.53\n",
      "✓ Sentiment analysis completed: 0.569\n",
      "4. Testing ML prediction...\n",
      "  Starting ML training with 183 data points...\n",
      "  Preparing ML data from 183 rows...\n",
      "    Found 51 numeric features\n",
      "    Handling missing values...\n",
      "      Removed 1 zero-variance features\n",
      "    Final dataset: 183 samples, 50 features\n",
      "    Target distribution: {1: 97, 0: 86}\n",
      "    Class balance OK: {1: 97, 0: 86}\n",
      "    Using test size: 0.30\n",
      "    Train: 128 samples, Test: 55 samples\n",
      "    Using full ensemble for large dataset (128 samples)\n",
      "      Training random_forest...\n",
      "        ✓ Train=0.984, Test=0.436\n",
      "        ⚠ High overfitting: 0.548\n",
      "      Training gradient_boosting...\n",
      "        ✓ Train=1.000, Test=0.509\n",
      "        ⚠ High overfitting: 0.491\n",
      "      Training logistic_regression...\n",
      "        ✓ Train=0.742, Test=0.491\n",
      "      Training xgboost...\n",
      "        ✓ Train=1.000, Test=0.473\n",
      "        ⚠ High overfitting: 0.527\n",
      "  ✓ ML training complete: 4 models, accuracy=0.477\n",
      "✓ ML prediction successful\n",
      "  Ensemble accuracy: 0.477\n",
      "  Prediction probability: 0.594\n",
      "5. Generating recommendation...\n",
      "    Technical score: 5.5/10 (0.55)\n",
      "    ML weight: 0.19 (prob: 0.59, accuracy: 0.48)\n",
      "    Sentiment: 0.57\n",
      "    Final combined score: 0.464\n",
      "✓ Final recommendation: HOLD\n",
      "\n",
      "✅ Test completed successfully for BTC\n",
      "✅ Initial test passed. Proceeding with full analysis...\n",
      "\n",
      "Fetching S&P 500 symbols...\n",
      "Found 503 S&P 500 symbols\n",
      "Will analyze 10 stocks from S&P 500\n",
      "Starting analysis of 1 stocks...\n",
      "\n",
      "[1/1] Processing AAPL...\n",
      "Analyzing AAPL...\n",
      "  Starting with 246 days of data\n",
      "  Calculating indicators for 246 days...\n",
      "  Before NaN removal: 246 rows\n",
      "    Dropped 1 rows due to NaN in Daily_Return\n",
      "    Dropped 12 rows due to NaN in RSI\n",
      "    Dropped 6 rows due to NaN in SMA20\n",
      "  After processing: 227 rows with 57 features\n",
      "  Analyzing sentiment for AAPL...\n",
      "    Trying yahoo_finance...\n",
      "    ⚠ No articles from yahoo_finance\n",
      "    Trying finviz...\n",
      "    ✓ Found 5 articles from finviz\n",
      "    News sentiment: 0.00 (from 5 articles)\n",
      "    Found Finviz analyst rating: 2.06 -> 0.97\n",
      "    Analyst sentiment: 0.97\n",
      "    Technical sentiment: 0.03\n",
      "    Combined sentiment: 0.50\n",
      "  Starting ML training with 227 data points...\n",
      "  Preparing ML data from 227 rows...\n",
      "    Found 54 numeric features\n",
      "    Handling missing values...\n",
      "      Removed 1 zero-variance features\n",
      "    Final dataset: 227 samples, 53 features\n",
      "    Target distribution: {1: 127, 0: 100}\n",
      "    Class balance OK: {1: 127, 0: 100}\n",
      "    Using test size: 0.30\n",
      "    Train: 158 samples, Test: 69 samples\n",
      "    Using full ensemble for large dataset (158 samples)\n",
      "      Training random_forest...\n",
      "        ✓ Train=0.987, Test=0.565\n",
      "        ⚠ High overfitting: 0.422\n",
      "      Training gradient_boosting...\n",
      "        ✓ Train=1.000, Test=0.493\n",
      "        ⚠ High overfitting: 0.507\n",
      "      Training logistic_regression...\n",
      "        ✓ Train=0.709, Test=0.580\n",
      "      Training xgboost...\n",
      "        ✓ Train=1.000, Test=0.580\n",
      "        ⚠ High overfitting: 0.420\n",
      "  ✓ ML training complete: 4 models, accuracy=0.554\n",
      "    Technical score: 0.375/10 (0.04)\n",
      "    ML weight: -0.00 (prob: 0.50, accuracy: 0.55)\n",
      "    Sentiment: 0.52\n",
      "    Final combined score: 0.147\n",
      "✓ AAPL: HOLD (Sentiment: 0.52)\n",
      "\n",
      "📊 Analysis Summary:\n",
      "  ✅ Successful: 1\n",
      "  ❌ Failed: 0\n",
      "  📈 Success Rate: 100.0%\n",
      "\n",
      "📈 Analysis Results:\n",
      "Successfully analyzed 1 stocks\n",
      "\n",
      "Recommendation Distribution:\n",
      "Recommendation\n",
      "HOLD    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "💾 Results saved to: ./results\\sp500_analysis_20250521_115105.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Last_Price</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ML_Probability</th>\n",
       "      <th>ML_Accuracy</th>\n",
       "      <th>Recommendation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>206.860001</td>\n",
       "      <td>43.441106</td>\n",
       "      <td>0.659609</td>\n",
       "      <td>0.517623</td>\n",
       "      <td>0.49764</td>\n",
       "      <td>0.554348</td>\n",
       "      <td>HOLD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol  Last_Price        RSI      MACD  Sentiment  ML_Probability  \\\n",
       "0   AAPL  206.860001  43.441106  0.659609   0.517623         0.49764   \n",
       "\n",
       "   ML_Accuracy Recommendation  \n",
       "0     0.554348           HOLD  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 Analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Main execution cell with enhanced error handling\n",
    "def analyze_multiple_stocks_robust(symbols, max_stocks=None):\n",
    "    \"\"\"Robust analysis of multiple stocks with error handling\"\"\"\n",
    "    if max_stocks:\n",
    "        symbols = symbols[:max_stocks]\n",
    "    \n",
    "    results = []\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    print(f\"Starting analysis of {len(symbols)} stocks...\")\n",
    "    \n",
    "    for i, symbol in enumerate(symbols):\n",
    "        print(f\"\\n[{i+1}/{len(symbols)}] Processing {symbol}...\")\n",
    "        \n",
    "        try:\n",
    "            result = quick_analyze_stock(symbol, show_details=False)\n",
    "            results.append(result)\n",
    "            \n",
    "            if result['Status'] == 'Success':\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"  ⚠ Failed: {result['Error']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Unexpected error: {e}\")\n",
    "            failed += 1\n",
    "            # Add failed entry to maintain count\n",
    "            results.append({\n",
    "                'Symbol': symbol,\n",
    "                'Status': 'Failed',\n",
    "                'Error': str(e),\n",
    "                'Recommendation': 'INSUFFICIENT_DATA'\n",
    "            })\n",
    "        \n",
    "        # Rate limiting with variable delays\n",
    "        if (i + 1) % 3 == 0 and i < len(symbols) - 1:\n",
    "            delay = random.randint(8, 15)\n",
    "            print(f\"  Pausing {delay}s to avoid rate limits...\")\n",
    "            time.sleep(delay)\n",
    "        elif i < len(symbols) - 1:\n",
    "            time.sleep(random.uniform(1, 3))  # Small random delay between requests\n",
    "    \n",
    "    print(f\"\\n📊 Analysis Summary:\")\n",
    "    print(f\"  ✅ Successful: {successful}\")\n",
    "    print(f\"  ❌ Failed: {failed}\")\n",
    "    print(f\"  📈 Success Rate: {successful/len(symbols)*100:.1f}%\")\n",
    "    \n",
    "    # Convert to DataFrame, only including successful analyses\n",
    "    successful_results = [r for r in results if r['Status'] == 'Success']\n",
    "    if successful_results:\n",
    "        df = pd.DataFrame(successful_results)\n",
    "        \n",
    "        # Clean up the DataFrame\n",
    "        columns_to_keep = [\n",
    "            'Symbol', 'Last_Price', 'RSI', 'MACD', 'Sentiment', \n",
    "            'ML_Probability', 'ML_Accuracy', 'Recommendation'\n",
    "        ]\n",
    "        existing_columns = [col for col in columns_to_keep if col in df.columns]\n",
    "        df = df[existing_columns]\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"⚠ No successful analyses to return\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\" or '__ipython__' in globals():\n",
    "    # Test with a single stock first\n",
    "    print(\"🧪 Running initial test...\")\n",
    "    test_success = test_single_stock_analysis(\"BTC\")\n",
    "    \n",
    "    if not test_success:\n",
    "        print(\"❌ Initial test failed. Please check the errors above.\")\n",
    "    else:\n",
    "        print(\"✅ Initial test passed. Proceeding with full analysis...\\n\")\n",
    "        \n",
    "        # Get S&P 500 symbols\n",
    "        sp500_symbols = get_sp500_symbols()\n",
    "        \n",
    "        # Get configuration\n",
    "        max_stocks = int(os.getenv('MAX_SYMBOLS', '10'))\n",
    "        print(f\"Will analyze {max_stocks} stocks from S&P 500\")\n",
    "\n",
    "        sp500_symbols = ['AAPL']\n",
    "        \n",
    "        # Run analysis\n",
    "        results_df = analyze_multiple_stocks_robust(sp500_symbols, max_stocks=max_stocks)\n",
    "        \n",
    "        if len(results_df) > 0:\n",
    "            print(f\"\\n📈 Analysis Results:\")\n",
    "            print(f\"Successfully analyzed {len(results_df)} stocks\")\n",
    "            \n",
    "            # Display summary statistics\n",
    "            print(f\"\\nRecommendation Distribution:\")\n",
    "            print(results_df['Recommendation'].value_counts())\n",
    "            \n",
    "            # Show top recommendations\n",
    "            buy_stocks = results_df[results_df['Recommendation'].isin(['STRONG_BUY', 'BUY'])]\n",
    "            if len(buy_stocks) > 0:\n",
    "                print(f\"\\n🟢 BUY Recommendations ({len(buy_stocks)}):\")\n",
    "                buy_display = buy_stocks.nlargest(5, 'ML_Probability') if 'ML_Probability' in buy_stocks.columns else buy_stocks.head(5)\n",
    "                for _, row in buy_display.iterrows():\n",
    "                    sentiment_str = f\"{row['Sentiment']:.2f}\" if pd.notna(row['Sentiment']) else \"N/A\"\n",
    "                    ml_str = f\"{row['ML_Probability']:.3f}\" if pd.notna(row.get('ML_Probability')) else \"N/A\"\n",
    "                    print(f\"  {row['Symbol']}: ${row['Last_Price']:.2f} | Sentiment: {sentiment_str} | ML: {ml_str} | {row['Recommendation']}\")\n",
    "            \n",
    "            sell_stocks = results_df[results_df['Recommendation'].isin(['STRONG_SELL', 'SELL'])]\n",
    "            if len(sell_stocks) > 0:\n",
    "                print(f\"\\n🔴 SELL Recommendations ({len(sell_stocks)}):\")\n",
    "                sell_display = sell_stocks.nsmallest(5, 'ML_Probability') if 'ML_Probability' in sell_stocks.columns else sell_stocks.head(5)\n",
    "                for _, row in sell_display.iterrows():\n",
    "                    sentiment_str = f\"{row['Sentiment']:.2f}\" if pd.notna(row['Sentiment']) else \"N/A\"\n",
    "                    ml_str = f\"{row['ML_Probability']:.3f}\" if pd.notna(row.get('ML_Probability')) else \"N/A\"\n",
    "                    print(f\"  {row['Symbol']}: ${row['Last_Price']:.2f} | Sentiment: {sentiment_str} | ML: {ml_str} | {row['Recommendation']}\")\n",
    "            \n",
    "            # Save results\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            output_file = os.path.join(output_dir, f\"sp500_analysis_{timestamp}.csv\")\n",
    "            results_df.to_csv(output_file, index=False)\n",
    "            print(f\"\\n💾 Results saved to: {output_file}\")\n",
    "            \n",
    "            # Create visualizations if we have enough data\n",
    "            if len(results_df) >= 3:\n",
    "                try:\n",
    "                    buy_stocks, sell_stocks = plot_results(results_df)\n",
    "                    styled_df = save_and_display_results(results_df)\n",
    "                    display(styled_df)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠ Visualization failed: {e}\")\n",
    "                    # Just display the basic DataFrame\n",
    "                    display(results_df)\n",
    "            else:\n",
    "                display(results_df)\n",
    "                \n",
    "            print(\"\\n🎉 Analysis completed successfully!\")\n",
    "        else:\n",
    "            print(\"\\n❌ No successful stock analyses. Please check the errors above.\")\n",
    "\n",
    "# Interactive single stock analysis (uncomment to use)\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(\"Interactive Single Stock Analysis\")\n",
    "# print(\"=\"*50)\n",
    "# interactive_single_stock_analysis(\"MSFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb3a45-51af-4dc4-9dd6-f2d7c8260213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
