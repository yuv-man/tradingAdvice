{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import required libraries and setup FinBERT\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from data_exploration import get_historical_data\n",
    "\n",
    "# Set device (CPU or GPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} for inference\")\n",
    "\n",
    "def setup_finbert():\n",
    "    \"\"\"Setup FinBERT model for financial sentiment analysis\"\"\"\n",
    "    try:\n",
    "        # Load FinBERT model for financial sentiment analysis\n",
    "        model_name = \"ProsusAI/finbert\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        \n",
    "        # Move model to GPU if available\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Create sentiment analysis pipeline\n",
    "        nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0 if device == \"cuda\" else -1)\n",
    "        print(\"✓ Successfully loaded FinBERT model\")\n",
    "        return nlp\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FinBERT: {e}\")\n",
    "        print(\"Falling back to alternative model...\")\n",
    "        \n",
    "        try:\n",
    "            # Fallback to another financial model or general sentiment model\n",
    "            model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"  # General sentiment model\n",
    "            nlp = pipeline(\"sentiment-analysis\", model=model_name, device=0 if device == \"cuda\" else -1)\n",
    "            print(\"✓ Successfully loaded fallback sentiment model\")\n",
    "            return nlp\n",
    "        except Exception as e2:\n",
    "            print(f\"Error loading fallback model: {e2}\")\n",
    "            print(\"Unable to load any sentiment model. Will use rule-based sentiment.\")\n",
    "            return None\n",
    "\n",
    "# Initialize sentiment model\n",
    "sentiment_model = setup_finbert()\n",
    "\n",
    "# Cell 2: News and sentiment analysis functions\n",
    "def get_stock_news_robust(symbol, max_articles=5):\n",
    "    news_data = []\n",
    "    \n",
    "    # Try multiple sources with different approaches\n",
    "    sources = [\n",
    "        ('yahoo_finance', get_yahoo_news),\n",
    "        ('finviz', get_finviz_news),\n",
    "        ('marketwatch', get_marketwatch_news),\n",
    "        ('seeking_alpha', get_seeking_alpha_news)\n",
    "    ]\n",
    "    \n",
    "    for source_name, source_func in sources:\n",
    "        if len(news_data) >= max_articles:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            print(f\"    Trying {source_name}...\")\n",
    "            source_news = source_func(symbol, max_articles - len(news_data))\n",
    "            if source_news:\n",
    "                news_data.extend(source_news)\n",
    "                print(f\"    ✓ Found {len(source_news)} articles from {source_name}\")\n",
    "            else:\n",
    "                print(f\"    ⚠ No articles from {source_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ❌ {source_name} failed: {e}\")\n",
    "        \n",
    "        # Small delay between sources\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # If no news found, create synthetic news entry for basic sentiment\n",
    "    if len(news_data) == 0:\n",
    "        print(f\"    Using fallback sentiment approach for {symbol}\")\n",
    "        # Create a neutral news entry to ensure we have something to analyze\n",
    "        news_data.append({\n",
    "            'headline': f'{symbol} market update',\n",
    "            'summary': f'Current market analysis for {symbol} shows mixed signals with ongoing price discovery.',\n",
    "            'source': 'synthetic'\n",
    "        })\n",
    "    return news_data\n",
    "\n",
    "def get_yahoo_news(symbol, max_articles=3):\n",
    "    news_data = []\n",
    "    \n",
    "    try:\n",
    "        url = f\"https://finance.yahoo.com/quote/{symbol}/news\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            return news_data\n",
    "            \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Multiple selectors to try for Yahoo Finance news\n",
    "        selectors = [\n",
    "            'li[data-testid=\"stream-item\"]',\n",
    "            'div[data-testid=\"card-container\"]',\n",
    "            'li.js-stream-content',\n",
    "            'div.Mb\\\\(14px\\\\)',\n",
    "            'div[data-test-locator=\"stream-item\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            items = soup.select(selector)\n",
    "            if items:\n",
    "                print(f\"      Found {len(items)} items with selector: {selector}\")\n",
    "                break\n",
    "        \n",
    "        for item in items[:max_articles]:\n",
    "            try:\n",
    "                # Try multiple ways to get the headline\n",
    "                headline_selectors = [\n",
    "                    'h3', 'h4', 'h5',\n",
    "                    'a[data-testid=\"clamp-container\"]',\n",
    "                    '.C\\\\(\\\\$c-link\\\\)',\n",
    "                    'div[data-testid=\"clamp-container\"] a',\n",
    "                    'a[href*=\"/news/\"]'\n",
    "                ]\n",
    "                \n",
    "                headline = None\n",
    "                for h_selector in headline_selectors:\n",
    "                    h_element = item.select_one(h_selector)\n",
    "                    if h_element and h_element.get_text().strip():\n",
    "                        headline = h_element.get_text().strip()\n",
    "                        break\n",
    "                \n",
    "                if headline and len(headline) > 10:  # Only valid headlines\n",
    "                    # Try to get summary\n",
    "                    summary_selectors = ['p', 'div.E\\\\(n\\\\)', 'span.C\\\\(\\\\$c-fuji-grey-l\\\\)']\n",
    "                    summary = \"\"\n",
    "                    for s_selector in summary_selectors:\n",
    "                        s_element = item.select_one(s_selector)\n",
    "                        if s_element:\n",
    "                            summary = s_element.get_text().strip()\n",
    "                            if len(summary) > 20:  # Only meaningful summaries\n",
    "                                break\n",
    "                    \n",
    "                    news_data.append({\n",
    "                        'headline': headline,\n",
    "                        'summary': summary,\n",
    "                        'source': 'yahoo_finance'\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "    return news_data\n",
    "\n",
    "def get_finviz_news(symbol, max_articles=3):\n",
    "    \"\"\"Get news from Finviz\"\"\"\n",
    "    news_data = []\n",
    "    \n",
    "    try:\n",
    "        url = f\"https://finviz.com/quote.ashx?t={symbol}\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            return news_data\n",
    "            \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find news table in Finviz\n",
    "        news_table = soup.find('table', {'class': 'fullview-news-outer'})\n",
    "        if not news_table:\n",
    "            news_table = soup.find('table', id='news-table')\n",
    "        \n",
    "        if news_table:\n",
    "            rows = news_table.find_all('tr')\n",
    "            for row in rows[:max_articles]:\n",
    "                try:\n",
    "                    link_tag = row.find('a')\n",
    "                    if link_tag:\n",
    "                        headline = link_tag.get_text().strip()\n",
    "                        if headline and len(headline) > 10:\n",
    "                            news_data.append({\n",
    "                                'headline': headline,\n",
    "                                'summary': '',\n",
    "                                'source': 'finviz'\n",
    "                            })\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "    return news_data\n",
    "\n",
    "def get_marketwatch_news(symbol, max_articles=2):\n",
    "    \"\"\"Get news from MarketWatch\"\"\"\n",
    "    news_data = []\n",
    "    \n",
    "    try:\n",
    "        url = f\"https://www.marketwatch.com/investing/stock/{symbol.lower()}\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            return news_data\n",
    "            \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Try different selectors for MarketWatch\n",
    "        selectors = [\n",
    "            '.element--article',\n",
    "            '.article-wrap',\n",
    "            '.headline',\n",
    "            'h3.no-margin'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            items = soup.select(selector)\n",
    "            if items:\n",
    "                break\n",
    "        \n",
    "        for item in items[:max_articles]:\n",
    "            try:\n",
    "                if selector == '.element--article':\n",
    "                    headline_tag = item.select_one('.headline') or item.select_one('h3')\n",
    "                else:\n",
    "                    headline_tag = item\n",
    "                    \n",
    "                if headline_tag:\n",
    "                    headline = headline_tag.get_text().strip()\n",
    "                    if headline and len(headline) > 10:\n",
    "                        news_data.append({\n",
    "                            'headline': headline,\n",
    "                            'summary': '',\n",
    "                            'source': 'marketwatch'\n",
    "                        })\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "    return news_data\n",
    "\n",
    "def get_seeking_alpha_news(symbol, max_articles=2):\n",
    "    \"\"\"Get news from Seeking Alpha\"\"\"\n",
    "    news_data = []\n",
    "    \n",
    "    try:\n",
    "        # Seeking Alpha has anti-bot measures, so this is a basic attempt\n",
    "        url = f\"https://seekingalpha.com/symbol/{symbol}/news\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=8)\n",
    "        if response.status_code != 200:\n",
    "            return news_data\n",
    "            \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Try to find article titles\n",
    "        article_links = soup.find_all('a', href=True)\n",
    "        for link in article_links[:max_articles * 3]:  # Check more links to filter\n",
    "            try:\n",
    "                if '/article/' in link['href'] or '/news/' in link['href']:\n",
    "                    headline = link.get_text().strip()\n",
    "                    if headline and len(headline) > 15 and len(headline) < 200:\n",
    "                        news_data.append({\n",
    "                            'headline': headline,\n",
    "                            'summary': '',\n",
    "                            'source': 'seeking_alpha'\n",
    "                        })\n",
    "                        if len(news_data) >= max_articles:\n",
    "                            break\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "    return news_data\n",
    "\n",
    "def get_analyst_ratings(symbol):\n",
    "    \"\"\"Get analyst ratings for a stock symbol with multiple fallback methods\"\"\"\n",
    "    \n",
    "    # Method 1: Try Yahoo Finance analysis page\n",
    "    try:\n",
    "        url = f\"https://finance.yahoo.com/quote/{symbol}/analysis\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Look for recommendation data in various places\n",
    "            # Method 1a: Look for recommendation table\n",
    "            tables = soup.find_all('table')\n",
    "            for table in tables:\n",
    "                rows = table.find_all('tr')\n",
    "                for row in rows:\n",
    "                    cells = row.find_all('td')\n",
    "                    if len(cells) >= 2:\n",
    "                        for i, cell in enumerate(cells):\n",
    "                            text = cell.get_text().lower()\n",
    "                            if 'recommendation' in text or 'mean' in text:\n",
    "                                # Look for number in next cells\n",
    "                                for j in range(i+1, min(len(cells), i+3)):\n",
    "                                    try:\n",
    "                                        rating_text = cells[j].get_text().strip()\n",
    "                                        rating = float(rating_text)\n",
    "                                        if 1.0 <= rating <= 5.0:\n",
    "                                            # Convert to our scale (-1 to 1)\n",
    "                                            normalized = 2 - (rating * 0.5)\n",
    "                                            print(f\"    Found analyst rating from table: {rating} -> {normalized:.2f}\")\n",
    "                                            return normalized\n",
    "                                    except (ValueError, IndexError):\n",
    "                                        continue\n",
    "            \n",
    "            # Method 1b: Look for recommendation in spans/divs\n",
    "            recommendation_keywords = ['strong buy', 'buy', 'hold', 'sell', 'strong sell']\n",
    "            spans = soup.find_all(['span', 'div', 'td'])\n",
    "            \n",
    "            for span in spans:\n",
    "                text = span.get_text().lower().strip()\n",
    "                if any(keyword in text for keyword in recommendation_keywords):\n",
    "                    # Try to find a number nearby\n",
    "                    parent = span.parent\n",
    "                    if parent:\n",
    "                        parent_text = parent.get_text()\n",
    "                        import re\n",
    "                        numbers = re.findall(r'\\d+\\.?\\d*', parent_text)\n",
    "                        for num_str in numbers:\n",
    "                            try:\n",
    "                                num = float(num_str)\n",
    "                                if 1.0 <= num <= 5.0:\n",
    "                                    normalized = 2 - (num * 0.5)\n",
    "                                    print(f\"    Found analyst rating from text: {num} -> {normalized:.2f}\")\n",
    "                                    return normalized\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                    \n",
    "                    # Map text-based recommendations\n",
    "                    if 'strong buy' in text:\n",
    "                        print(\"    Found 'strong buy' recommendation\")\n",
    "                        return 1.0\n",
    "                    elif 'buy' in text and 'strong' not in text:\n",
    "                        print(\"    Found 'buy' recommendation\")\n",
    "                        return 0.75\n",
    "                    elif 'hold' in text:\n",
    "                        print(\"    Found 'hold' recommendation\")\n",
    "                        return 0.0\n",
    "                    elif 'strong sell' in text:\n",
    "                        print(\"    Found 'strong sell' recommendation\")\n",
    "                        return -1.0\n",
    "                    elif 'sell' in text and 'strong' not in text:\n",
    "                        print(\"    Found 'sell' recommendation\")\n",
    "                        return -0.75\n",
    "            \n",
    "            # Method 1c: Look in script tags for JSON data\n",
    "            scripts = soup.find_all('script')\n",
    "            for script in scripts:\n",
    "                if script.string:\n",
    "                    script_text = script.string\n",
    "                    # Look for recommendation patterns in JavaScript\n",
    "                    import re\n",
    "                    patterns = [\n",
    "                        r'\"recommendationMean\":\\s*{\\s*\"raw\":\\s*([0-9.]+)',\n",
    "                        r'\"recommendationMean\":\\s*([0-9.]+)',\n",
    "                        r'recommendationMean[\"\\']:\\s*([0-9.]+)',\n",
    "                        r'recommendation[\"\\']:\\s*([0-9.]+)'\n",
    "                    ]\n",
    "                    \n",
    "                    for pattern in patterns:\n",
    "                        match = re.search(pattern, script_text)\n",
    "                        if match:\n",
    "                            try:\n",
    "                                rating = float(match.group(1))\n",
    "                                if 1.0 <= rating <= 5.0:\n",
    "                                    normalized = 2 - (rating * 0.5)\n",
    "                                    print(f\"    Found analyst rating from script: {rating} -> {normalized:.2f}\")\n",
    "                                    return normalized\n",
    "                            except (ValueError, IndexError):\n",
    "                                continue\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"    Yahoo Finance analyst method failed: {e}\")\n",
    "    \n",
    "    # Method 2: Try Finviz for analyst data\n",
    "    try:\n",
    "        url = f\"https://finviz.com/quote.ashx?t={symbol}\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=8)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Look for recommendation in Finviz tables\n",
    "            tables = soup.find_all('table')\n",
    "            for table in tables:\n",
    "                cells = table.find_all('td')\n",
    "                for i, cell in enumerate(cells):\n",
    "                    text = cell.get_text().strip()\n",
    "                    if 'Recom' in text or 'Recommendation' in text:\n",
    "                        # Look for value in adjacent cells\n",
    "                        try:\n",
    "                            if i + 1 < len(cells):\n",
    "                                rec_text = cells[i + 1].get_text().strip()\n",
    "                                try:\n",
    "                                    rating = float(rec_text)\n",
    "                                    if 1.0 <= rating <= 5.0:\n",
    "                                        normalized = 2 - (rating * 0.5)\n",
    "                                        print(f\"    Found Finviz analyst rating: {rating} -> {normalized:.2f}\")\n",
    "                                        return normalized\n",
    "                                except ValueError:\n",
    "                                    pass\n",
    "                        except IndexError:\n",
    "                            pass\n",
    "                            \n",
    "    except Exception as e:\n",
    "        print(f\"    Finviz analyst method failed: {e}\")\n",
    "    \n",
    "    # Method 3: Create synthetic analyst sentiment based on price momentum\n",
    "    # If all else fails, estimate analyst sentiment from recent price performance\n",
    "    try:\n",
    "        # Get basic stock info to estimate sentiment\n",
    "        url = f\"https://finance.yahoo.com/quote/{symbol}\"\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Look for price change information\n",
    "            spans = soup.find_all('span')\n",
    "            for span in spans:\n",
    "                text = span.get_text()\n",
    "                if '%' in text and ('+' in text or '-' in text):\n",
    "                    try:\n",
    "                        # Extract percentage change\n",
    "                        import re\n",
    "                        pct_match = re.search(r'([-+]?)([0-9.]+)%', text)\n",
    "                        if pct_match:\n",
    "                            sign = pct_match.group(1)\n",
    "                            value = float(pct_match.group(2))\n",
    "                            pct_change = value if sign != '-' else -value\n",
    "                            \n",
    "                            # Convert percentage change to sentiment\n",
    "                            # +5% or more = positive, -5% or less = negative\n",
    "                            if pct_change > 5:\n",
    "                                sentiment = 0.5\n",
    "                            elif pct_change > 2:\n",
    "                                sentiment = 0.25\n",
    "                            elif pct_change < -5:\n",
    "                                sentiment = -0.5\n",
    "                            elif pct_change < -2:\n",
    "                                sentiment = -0.25\n",
    "                            else:\n",
    "                                sentiment = 0.0\n",
    "                            \n",
    "                            print(f\"    Estimated analyst sentiment from price change ({pct_change:.1f}%): {sentiment:.2f}\")\n",
    "                            return sentiment\n",
    "                    except (ValueError, AttributeError):\n",
    "                        continue\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"    Fallback analyst method failed: {e}\")\n",
    "    \n",
    "    print(\"    All analyst sentiment methods failed, returning neutral\")\n",
    "    return 0.0  # Neutral if not found\n",
    "\n",
    "def analyze_news_sentiment_rule_based(news_data):\n",
    "    \"\"\"Simple rule-based sentiment analysis as fallback\"\"\"\n",
    "    if not news_data:\n",
    "        return 0.0\n",
    "    \n",
    "    # Define positive and negative word lists for financial context\n",
    "    positive_words = [\n",
    "        'gain', 'gains', 'up', 'rise', 'rises', 'rising', 'rose', 'bullish', 'outperform',\n",
    "        'buy', 'growth', 'profit', 'profits', 'positive', 'strong', 'strength', 'higher',\n",
    "        'record', 'upgrade', 'upgraded', 'beat', 'beats', 'exceed', 'exceeds', 'success',\n",
    "        'successful', 'increase', 'increases', 'increased', 'boost', 'boosts', 'boosted',\n",
    "        'opportunity', 'opportunities', 'potential', 'promising', 'optimistic', 'confident',\n",
    "        'momentum', 'surge', 'rally', 'advance', 'advances', 'breakthrough', 'expansion',\n",
    "        'outperforming', 'soars', 'jumped', 'climbed', 'accelerate', 'accelerating'\n",
    "    ]\n",
    "    \n",
    "    negative_words = [\n",
    "        'loss', 'losses', 'down', 'fall', 'falls', 'falling', 'fell', 'bearish', 'underperform',\n",
    "        'sell', 'decline', 'declines', 'declined', 'negative', 'weak', 'weakness', 'lower',\n",
    "        'downgrade', 'downgraded', 'miss', 'misses', 'missed', 'fail', 'fails', 'failed',\n",
    "        'decrease', 'decreases', 'decreased', 'cut', 'cuts', 'risk', 'risks', 'risky',\n",
    "        'concern', 'concerns', 'warning', 'problem', 'problems', 'threat', 'threats',\n",
    "        'disappointing', 'crash', 'plunge', 'plummeted', 'volatile', 'uncertainty',\n",
    "        'recession', 'bear', 'correction', 'selloff', 'slump', 'struggle', 'struggling'\n",
    "    ]\n",
    "    \n",
    "    # Intensifier words that modify sentiment\n",
    "    intensifiers = {\n",
    "        'very': 1.5, 'extremely': 2.0, 'significantly': 1.7, 'substantially': 1.7,\n",
    "        'dramatically': 2.0, 'sharply': 1.8, 'strongly': 1.6, 'heavily': 1.5,\n",
    "        'massive': 2.0, 'huge': 1.8, 'major': 1.5, 'significant': 1.3\n",
    "    }\n",
    "    \n",
    "    total_sentiment = 0\n",
    "    article_count = 0\n",
    "    \n",
    "    for article in news_data:\n",
    "        text = (article['headline'] + \" \" + article.get('summary', '')).lower()\n",
    "        words = text.split()\n",
    "        \n",
    "        # Count positive and negative words\n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            # Clean the word\n",
    "            clean_word = word.strip('.,!?;:\"()[]')\n",
    "            \n",
    "            # Check for intensifiers\n",
    "            intensifier = 1.0\n",
    "            if i > 0:\n",
    "                prev_word = words[i-1].strip('.,!?;:\"()[]')\n",
    "                if prev_word in intensifiers:\n",
    "                    intensifier = intensifiers[prev_word]\n",
    "            \n",
    "            # Count sentiment words with intensifier\n",
    "            if clean_word in positive_words:\n",
    "                positive_count += intensifier\n",
    "            elif clean_word in negative_words:\n",
    "                negative_count += intensifier\n",
    "        \n",
    "        # Calculate sentiment for this article\n",
    "        if positive_count > 0 or negative_count > 0:\n",
    "            # Normalize by total sentiment words found\n",
    "            total_sentiment_words = positive_count + negative_count\n",
    "            article_sentiment = (positive_count - negative_count) / total_sentiment_words\n",
    "            \n",
    "            # Scale to reasonable range\n",
    "            article_sentiment = max(-1.0, min(1.0, article_sentiment))\n",
    "            total_sentiment += article_sentiment\n",
    "            article_count += 1\n",
    "    \n",
    "    # Average sentiment across all articles\n",
    "    if article_count > 0:\n",
    "        average_sentiment = total_sentiment / article_count\n",
    "        return average_sentiment\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def analyze_news_sentiment_with_finbert(news_data, nlp):\n",
    "    if not news_data:\n",
    "        return 0.0  # Neutral sentiment if no news\n",
    "    \n",
    "    # If FinBERT model is not available, use rule-based approach\n",
    "    if nlp is None:\n",
    "        print(\"    Using rule-based sentiment analysis (FinBERT not available)\")\n",
    "        return analyze_news_sentiment_rule_based(news_data)\n",
    "    \n",
    "    sentiments = []\n",
    "    \n",
    "    for article in news_data:\n",
    "        text = article['headline']\n",
    "        if len(article.get('summary', '')) > 0:\n",
    "            text += \" \" + article['summary']\n",
    "        \n",
    "        # Skip empty text\n",
    "        if not text.strip():\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Try FinBERT analysis\n",
    "            result = nlp(text[:500])  # Limit text length\n",
    "            \n",
    "            if result and len(result) > 0:\n",
    "                # Map FinBERT sentiment labels to scores\n",
    "                sent_label = result[0]['label'].lower()\n",
    "                sent_score = result[0]['score']\n",
    "                \n",
    "                if 'positive' in sent_label:\n",
    "                    sentiments.append(sent_score)\n",
    "                elif 'negative' in sent_label:\n",
    "                    sentiments.append(-sent_score)\n",
    "                else:  # neutral\n",
    "                    sentiments.append(0.0)\n",
    "            else:\n",
    "                # Fallback to rule-based for this article\n",
    "                rule_sentiment = analyze_news_sentiment_rule_based([article])\n",
    "                sentiments.append(rule_sentiment)\n",
    "                \n",
    "        except Exception as e:\n",
    "            # If FinBERT fails, use rule-based for this article\n",
    "            try:\n",
    "                rule_sentiment = analyze_news_sentiment_rule_based([article])\n",
    "                sentiments.append(rule_sentiment)\n",
    "            except:\n",
    "                sentiments.append(0.0)\n",
    "    \n",
    "    # Average all sentiment scores\n",
    "    if sentiments:\n",
    "        avg_sentiment = sum(sentiments) / len(sentiments)\n",
    "        # Scale to ensure [-1, 1] range\n",
    "        return max(-1.0, min(1.0, avg_sentiment))\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def get_stock_news(symbol, max_articles=5):\n",
    "    \"\"\"Get recent news articles about a stock symbol\"\"\"\n",
    "    return get_stock_news_robust(symbol, max_articles)\n",
    "\n",
    "def get_sentiment_analysis(symbol, data=None):\n",
    "    \"\"\"Get comprehensive sentiment analysis for a stock using FinBERT\"\"\"\n",
    "    print(f\"  Analyzing sentiment for {symbol}...\")\n",
    "    \n",
    "    # Initialize sentiment components\n",
    "    news_sentiment = 0.0\n",
    "    analyst_sentiment = 0.0\n",
    "    tech_sentiment = 0.0\n",
    "    \n",
    "    # Get news sentiment using FinBERT\n",
    "    try:\n",
    "        news_data = get_stock_news(symbol)\n",
    "        news_sentiment = analyze_news_sentiment_with_finbert(news_data, sentiment_model)\n",
    "        print(f\"    News sentiment: {news_sentiment:.2f} (from {len(news_data)} articles)\")\n",
    "    except Exception as e:\n",
    "        print(f\"    News sentiment failed: {e}\")\n",
    "        news_sentiment = 0.0\n",
    "    \n",
    "    # Get analyst ratings\n",
    "    try:\n",
    "        analyst_sentiment = get_analyst_ratings(symbol)\n",
    "        print(f\"    Analyst sentiment: {analyst_sentiment:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Analyst sentiment failed: {e}\")\n",
    "        analyst_sentiment = 0.0\n",
    "    \n",
    "    # Get technical sentiment\n",
    "    try:\n",
    "        if data is not None:\n",
    "            tech_sentiment = get_technical_sentiment(data)\n",
    "            print(f\"    Technical sentiment: {tech_sentiment:.2f}\")\n",
    "        else:\n",
    "            tech_sentiment = 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"    Technical sentiment failed: {e}\")\n",
    "        tech_sentiment = 0.0\n",
    "    \n",
    "    # Weight the components with fallbacks\n",
    "    # If we have all three: 40% news, 30% analyst, 30% technical\n",
    "    # If missing news: 50% analyst, 50% technical\n",
    "    # If missing analyst: 60% news, 40% technical\n",
    "    # If only technical: 100% technical\n",
    "    \n",
    "    components = []\n",
    "    weights = []\n",
    "    \n",
    "    if abs(news_sentiment) > 0.001:  # We have meaningful news sentiment\n",
    "        components.append(news_sentiment)\n",
    "        weights.append(0.4)\n",
    "    \n",
    "    if abs(analyst_sentiment) > 0.001:  # We have meaningful analyst sentiment\n",
    "        components.append(analyst_sentiment)\n",
    "        weights.append(0.3)\n",
    "    \n",
    "    if abs(tech_sentiment) > 0.001:  # We have meaningful technical sentiment\n",
    "        components.append(tech_sentiment)\n",
    "        weights.append(0.3)\n",
    "    \n",
    "    # Normalize weights\n",
    "    if weights:\n",
    "        total_weight = sum(weights)\n",
    "        weights = [w/total_weight for w in weights]\n",
    "        combined_sentiment = sum(c*w for c, w in zip(components, weights))\n",
    "    else:\n",
    "        # Fallback to neutral sentiment\n",
    "        combined_sentiment = 0.0\n",
    "    \n",
    "    print(f\"    Combined sentiment: {combined_sentiment:.2f}\")\n",
    "    \n",
    "    # Add a small random variation to avoid all stocks having identical sentiment scores\n",
    "    final_sentiment = combined_sentiment + (random.uniform(-0.05, 0.05))\n",
    "    final_sentiment = max(-1.0, min(1.0, final_sentiment))\n",
    "    \n",
    "    # Add a small delay to avoid hitting rate limits\n",
    "    time.sleep(random.uniform(1, 2))\n",
    "    \n",
    "    return final_sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
