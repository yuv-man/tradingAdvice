{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import required libraries and functions\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cell 2: ML Model Training Functions\n",
    "def create_ensemble_models_adaptive(data_size):\n",
    "    \"\"\"Create models adapted to the size of available data\"\"\"\n",
    "    if data_size < 50:\n",
    "        # Very small dataset - use simple models\n",
    "        models = {\n",
    "            'logistic_regression': LogisticRegression(\n",
    "                random_state=42,\n",
    "                max_iter=100,\n",
    "                solver='liblinear',\n",
    "                C=0.1  # More regularization\n",
    "            )\n",
    "        }\n",
    "        print(f\"    Using simple model for small dataset ({data_size} samples)\")\n",
    "    elif data_size < 100:\n",
    "        # Small dataset - use regularized models\n",
    "        models = {\n",
    "            'random_forest': RandomForestClassifier(\n",
    "                n_estimators=20,\n",
    "                max_depth=5,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=5,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'logistic_regression': LogisticRegression(\n",
    "                random_state=42,\n",
    "                max_iter=200,\n",
    "                solver='liblinear',\n",
    "                C=0.1\n",
    "            )\n",
    "        }\n",
    "        print(f\"    Using regularized models for medium dataset ({data_size} samples)\")\n",
    "    else:\n",
    "        # Larger dataset - use full ensemble\n",
    "        models = {\n",
    "            'random_forest': RandomForestClassifier(\n",
    "                n_estimators=50,\n",
    "                max_depth=10,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'gradient_boosting': GradientBoostingClassifier(\n",
    "                n_estimators=50,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'logistic_regression': LogisticRegression(\n",
    "                random_state=42,\n",
    "                max_iter=500,\n",
    "                solver='liblinear',\n",
    "                C=1.0\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Add XGBoost for larger datasets\n",
    "        try:\n",
    "            models['xgboost'] = xgb.XGBClassifier(\n",
    "                n_estimators=50,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                random_state=42,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='logloss',\n",
    "                verbosity=0\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"    XGBoost not available: {e}\")\n",
    "        \n",
    "        print(f\"    Using full ensemble for large dataset ({data_size} samples)\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def prepare_ml_data_robust(data):\n",
    "    \"\"\"Prepare and clean data for ML training\"\"\"\n",
    "    try:\n",
    "        print(f\"  Preparing ML data from {len(data)} rows...\")\n",
    "        \n",
    "        # Select features for ML, excluding target and non-predictive columns\n",
    "        exclude_cols = [\n",
    "            'Target', 'Target_Return', 'Next_Close', \n",
    "            'Tech_Sentiment',  # Used separately as External_Sentiment\n",
    "        ]\n",
    "        \n",
    "        # Get all potential feature columns\n",
    "        all_feature_cols = [col for col in data.columns if col not in exclude_cols]\n",
    "        \n",
    "        # Keep only numeric columns\n",
    "        numeric_cols = []\n",
    "        for col in all_feature_cols:\n",
    "            if pd.api.types.is_numeric_dtype(data[col]):\n",
    "                numeric_cols.append(col)\n",
    "        \n",
    "        print(f\"    Found {len(numeric_cols)} numeric features\")\n",
    "        \n",
    "        # Create feature matrix and target\n",
    "        X = data[numeric_cols].copy()\n",
    "        y = data['Target'].copy()\n",
    "        \n",
    "        # Handle remaining NaN values more aggressively\n",
    "        print(f\"    Handling missing values...\")\n",
    "        \n",
    "        # Count NaN values per column\n",
    "        nan_counts = X.isna().sum()\n",
    "        \n",
    "        # Remove columns with too many NaN values (>50% missing)\n",
    "        threshold = len(X) * 0.5\n",
    "        good_cols = []\n",
    "        for col in X.columns:\n",
    "            if nan_counts[col] <= threshold:\n",
    "                good_cols.append(col)\n",
    "            else:\n",
    "                print(f\"      Removing {col}: {nan_counts[col]}/{len(X)} missing values\")\n",
    "        \n",
    "        X = X[good_cols]\n",
    "        \n",
    "        # Fill remaining NaN values with median\n",
    "        for col in X.columns:\n",
    "            if X[col].isna().any():\n",
    "                median_val = X[col].median()\n",
    "                if pd.isna(median_val):  # If median is also NaN, use 0\n",
    "                    median_val = 0\n",
    "                n_filled = X[col].isna().sum()\n",
    "                X[col].fillna(median_val, inplace=True)\n",
    "                print(f\"      Filled {n_filled} NaN values in {col} with {median_val:.3f}\")\n",
    "        \n",
    "        # Handle infinite values\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "        X = X.fillna(X.median())\n",
    "        X = X.fillna(0)  # Final fallback\n",
    "        \n",
    "        # Remove features with zero variance\n",
    "        from sklearn.feature_selection import VarianceThreshold\n",
    "        variance_selector = VarianceThreshold(threshold=0.0)\n",
    "        X_filtered = variance_selector.fit_transform(X)\n",
    "        selected_features = X.columns[variance_selector.get_support()].tolist()\n",
    "        X = pd.DataFrame(X_filtered, columns=selected_features, index=X.index)\n",
    "        \n",
    "        if len(selected_features) < len(numeric_cols):\n",
    "            removed = len(numeric_cols) - len(selected_features)\n",
    "            print(f\"      Removed {removed} zero-variance features\")\n",
    "        \n",
    "        # Ensure we have valid target values\n",
    "        valid_mask = y.notna()\n",
    "        X = X[valid_mask]\n",
    "        y = y[valid_mask]\n",
    "        \n",
    "        # Final data checks\n",
    "        print(f\"    Final dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "        \n",
    "        # Check target distribution\n",
    "        target_counts = y.value_counts()\n",
    "        print(f\"    Target distribution: {target_counts.to_dict()}\")\n",
    "        \n",
    "        return X, y, selected_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    Error preparing ML data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "def train_and_predict_robust(data):\n",
    "    \"\"\"Enhanced ML training with small dataset support\"\"\"\n",
    "    if data is None:\n",
    "        print(\"  No data provided for ML training\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Starting ML training with {len(data)} data points...\")\n",
    "    \n",
    "    # Lower the minimum data requirement\n",
    "    if len(data) < 20:\n",
    "        print(f\"  Insufficient data for ML training (need at least 20, got {len(data)})\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Prepare the data\n",
    "        X, y, feature_cols = prepare_ml_data_robust(data)\n",
    "        \n",
    "        if X is None or len(X) < 15:\n",
    "            print(\"  Failed to prepare sufficient data for ML\")\n",
    "            return None\n",
    "        \n",
    "        # Check class balance\n",
    "        class_counts = y.value_counts()\n",
    "        if len(class_counts) < 2:\n",
    "            print(f\"  Only one class present: {class_counts.to_dict()}\")\n",
    "            return None\n",
    "        \n",
    "        min_class_count = class_counts.min()\n",
    "        if min_class_count < 3:\n",
    "            print(f\"  Insufficient class balance for ML: {class_counts.to_dict()}\")\n",
    "            print(\"  (Need at least 3 samples of each class)\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"    Class balance OK: {class_counts.to_dict()}\")\n",
    "        \n",
    "        # Use adaptive test size based on total data\n",
    "        if len(X) < 30:\n",
    "            test_size = 0.2  # Keep more for training\n",
    "        elif len(X) < 60:\n",
    "            test_size = 0.25\n",
    "        else:\n",
    "            test_size = 0.3\n",
    "        \n",
    "        # Ensure minimum test size\n",
    "        min_test_samples = max(3, min_class_count // 2)\n",
    "        actual_test_size = max(test_size, min_test_samples / len(X))\n",
    "        \n",
    "        print(f\"    Using test size: {actual_test_size:.2f}\")\n",
    "        \n",
    "        # Split data\n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, \n",
    "                test_size=actual_test_size, \n",
    "                stratify=y, \n",
    "                random_state=42\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            print(f\"    Stratified split failed ({e}), using random split\")\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, \n",
    "                test_size=actual_test_size, \n",
    "                random_state=42\n",
    "            )\n",
    "        \n",
    "        print(f\"    Train: {len(X_train)} samples, Test: {len(X_test)} samples\")\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Get adaptive models\n",
    "        models = create_ensemble_models_adaptive(len(X_train))\n",
    "        model_results = {}\n",
    "        successful_models = 0\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            try:\n",
    "                print(f\"      Training {name}...\")\n",
    "                \n",
    "                # Train the model\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                \n",
    "                # Make predictions\n",
    "                if len(X_test) > 0:\n",
    "                    test_pred = model.predict(X_test_scaled)\n",
    "                    test_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "                    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "                else:\n",
    "                    # If no test set, use train set for evaluation (not ideal but better than failure)\n",
    "                    test_pred = model.predict(X_train_scaled)\n",
    "                    test_proba = model.predict_proba(X_train_scaled)[:, 1]\n",
    "                    test_accuracy = accuracy_score(y_train, test_pred)\n",
    "                    print(f\"        Warning: Using training set for evaluation\")\n",
    "                \n",
    "                # Train accuracy\n",
    "                train_pred = model.predict(X_train_scaled)\n",
    "                train_accuracy = accuracy_score(y_train, train_pred)\n",
    "                \n",
    "                # Check for overfitting\n",
    "                overfitting = train_accuracy - test_accuracy\n",
    "                \n",
    "                model_results[name] = {\n",
    "                    'model': model,\n",
    "                    'train_accuracy': train_accuracy,\n",
    "                    'test_accuracy': test_accuracy,\n",
    "                    'prediction': test_pred[-1] if len(test_pred) > 0 else train_pred[-1],\n",
    "                    'probability': test_proba[-1] if len(test_proba) > 0 else 0.5,\n",
    "                    'overfitting': overfitting\n",
    "                }\n",
    "                \n",
    "                successful_models += 1\n",
    "                print(f\"        ✓ Train={train_accuracy:.3f}, Test={test_accuracy:.3f}\")\n",
    "                \n",
    "                if overfitting > 0.3:\n",
    "                    print(f\"        ⚠ High overfitting: {overfitting:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"        ❌ {name} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if successful_models == 0:\n",
    "            print(\"  All ML models failed to train\")\n",
    "            return None\n",
    "        \n",
    "        # Ensemble prediction\n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "        weights = []\n",
    "        \n",
    "        for name, result in model_results.items():\n",
    "            # Weight by test accuracy, penalize overfitting\n",
    "            weight = result['test_accuracy'] * (1 - min(result['overfitting'], 0.5))\n",
    "            weight = max(weight, 0.1)  # Minimum weight\n",
    "            weights.append(weight)\n",
    "            predictions.append(result['prediction'])\n",
    "            probabilities.append(result['probability'])\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        weights = np.array(weights)\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        ensemble_prediction = np.average(predictions, weights=weights)\n",
    "        ensemble_probability = np.average(probabilities, weights=weights)\n",
    "        average_accuracy = np.average([r['test_accuracy'] for r in model_results.values()])\n",
    "        \n",
    "        # Get feature importance from best model\n",
    "        best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['test_accuracy'])\n",
    "        best_model = model_results[best_model_name]['model']\n",
    "        \n",
    "        importance_dict = {}\n",
    "        if hasattr(best_model, 'feature_importances_'):\n",
    "            importance_dict[best_model_name] = dict(zip(feature_cols, best_model.feature_importances_))\n",
    "        elif hasattr(best_model, 'coef_'):\n",
    "            importance_dict[best_model_name] = dict(zip(feature_cols, np.abs(best_model.coef_[0])))\n",
    "        \n",
    "        result = {\n",
    "            'prediction': ensemble_prediction,\n",
    "            'probability': ensemble_probability,\n",
    "            'accuracy': average_accuracy,\n",
    "            'model_results': model_results,\n",
    "            'feature_importance': importance_dict,\n",
    "            'successful_models': successful_models,\n",
    "            'best_model': best_model_name,\n",
    "            'data_size': len(X)\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✓ ML training complete: {successful_models} models, accuracy={average_accuracy:.3f}\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ML training error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def train_and_predict(data):\n",
    "    \"\"\"Main ML training function with enhanced robustness for small datasets\"\"\"\n",
    "    return train_and_predict_robust(data)\n",
    "\n",
    "def generate_enhanced_recommendation(technical_data, sentiment_score, ml_result):\n",
    "    \"\"\"Generate trading recommendations\"\"\"\n",
    "    if technical_data is None or len(technical_data) < 5:\n",
    "        return \"INSUFFICIENT_DATA\"\n",
    "    \n",
    "    latest = technical_data.iloc[-1]\n",
    "    \n",
    "    # Technical analysis score (more detailed)\n",
    "    technical_score = 0\n",
    "    \n",
    "    # Moving Average signals (stronger weight for clear trends)\n",
    "    ma_signals = 0\n",
    "    ma_count = 0\n",
    "    \n",
    "    # Check available moving averages\n",
    "    if 'SMA20' in latest.index and pd.notna(latest['SMA20']):\n",
    "        if latest['Close'] > latest['SMA20']:\n",
    "            ma_signals += 1\n",
    "        else:\n",
    "            ma_signals -= 1\n",
    "        ma_count += 1\n",
    "    \n",
    "    if 'SMA50' in latest.index and pd.notna(latest['SMA50']):\n",
    "        if latest['Close'] > latest['SMA50']:\n",
    "            ma_signals += 1.5  # More weight for longer MA\n",
    "        else:\n",
    "            ma_signals -= 1.5\n",
    "        ma_count += 1\n",
    "    \n",
    "    if 'SMA200' in latest.index and pd.notna(latest['SMA200']):\n",
    "        if latest['Close'] > latest['SMA200']:\n",
    "            ma_signals += 2  # Strong weight for long-term trend\n",
    "        else:\n",
    "            ma_signals -= 2\n",
    "        ma_count += 1\n",
    "    \n",
    "    # Check MA crossovers\n",
    "    if 'SMA20' in latest.index and 'SMA50' in latest.index:\n",
    "        if pd.notna(latest['SMA20']) and pd.notna(latest['SMA50']):\n",
    "            if latest['SMA20'] > latest['SMA50']:\n",
    "                ma_signals += 1\n",
    "            else:\n",
    "                ma_signals -= 1\n",
    "            ma_count += 1\n",
    "    \n",
    "    # Normalize MA signals\n",
    "    if ma_count > 0:\n",
    "        technical_score += ma_signals / ma_count * 3  # Scale up the MA influence\n",
    "    \n",
    "    # Momentum signals (RSI)\n",
    "    if 'RSI' in latest.index and pd.notna(latest['RSI']):\n",
    "        rsi = latest['RSI']\n",
    "        if rsi < 25:  # Very oversold - strong buy signal\n",
    "            technical_score += 3\n",
    "        elif rsi < 35:  # Oversold - buy signal\n",
    "            technical_score += 2\n",
    "        elif rsi > 75:  # Very overbought - strong sell signal\n",
    "            technical_score -= 3\n",
    "        elif rsi > 65:  # Overbought - sell signal\n",
    "            technical_score -= 2\n",
    "        # Neutral zone (35-65) adds no score\n",
    "    \n",
    "    # MACD signals\n",
    "    if 'MACD' in latest.index and 'Signal_Line' in latest.index:\n",
    "        if pd.notna(latest['MACD']) and pd.notna(latest['Signal_Line']):\n",
    "            if latest['MACD'] > latest['Signal_Line']:\n",
    "                technical_score += 2\n",
    "            else:\n",
    "                technical_score -= 2\n",
    "    \n",
    "    # Trend strength (ADX)\n",
    "    if 'ADX' in latest.index and pd.notna(latest['ADX']):\n",
    "        adx = latest['ADX']\n",
    "        if adx > 40:  # Strong trend\n",
    "            # Determine trend direction from price vs MAs\n",
    "            if 'SMA20' in latest.index and pd.notna(latest['SMA20']):\n",
    "                if latest['Close'] > latest['SMA20']:\n",
    "                    technical_score += 1  # Strong uptrend\n",
    "                else:\n",
    "                    technical_score -= 1  # Strong downtrend\n",
    "    \n",
    "    # Price momentum (recent performance)\n",
    "    if 'ROC10' in latest.index and pd.notna(latest['ROC10']):\n",
    "        roc = latest['ROC10']\n",
    "        if roc > 5:  # Strong positive momentum\n",
    "            technical_score += 2\n",
    "        elif roc > 2:  # Positive momentum\n",
    "            technical_score += 1\n",
    "        elif roc < -5:  # Strong negative momentum\n",
    "            technical_score -= 2\n",
    "        elif roc < -2:  # Negative momentum\n",
    "            technical_score -= 1\n",
    "    \n",
    "    # Normalize technical score (-10 to +10 range)\n",
    "    technical_score = max(-10, min(10, technical_score))\n",
    "    normalized_technical = technical_score / 10.0  # Scale to -1 to 1\n",
    "    \n",
    "    print(f\"    Technical score: {technical_score}/10 ({normalized_technical:.2f})\")\n",
    "    \n",
    "    # ML prediction weight\n",
    "    ml_weight = 0\n",
    "    ml_confidence = 0.5\n",
    "    \n",
    "    if ml_result:\n",
    "        # Scale ML probability to -1 to 1 range\n",
    "        ml_weight = (ml_result['probability'] - 0.5) * 2\n",
    "        ml_confidence = ml_result.get('accuracy', 0.5)\n",
    "        print(f\"    ML weight: {ml_weight:.2f} (prob: {ml_result['probability']:.2f}, accuracy: {ml_confidence:.2f})\")\n",
    "    \n",
    "    # Sentiment weight (already in -1 to 1 range)\n",
    "    print(f\"    Sentiment: {sentiment_score:.2f}\")\n",
    "    \n",
    "    # Combine all scores with weights\n",
    "    # Increased weights to make system more responsive\n",
    "    if ml_confidence >= 0.6:\n",
    "        # High ML confidence: 35% technical, 45% ML, 20% sentiment\n",
    "        final_score = (normalized_technical * 0.35) + (ml_weight * 0.45) + (sentiment_score * 0.20)\n",
    "    else:\n",
    "        # Low ML confidence: 50% technical, 25% ML, 25% sentiment\n",
    "        final_score = (normalized_technical * 0.50) + (ml_weight * 0.25) + (sentiment_score * 0.25)\n",
    "    \n",
    "    print(f\"    Final combined score: {final_score:.3f}\")\n",
    "    \n",
    "    # Generate recommendation with more aggressive thresholds\n",
    "    if ml_confidence < 0.5:\n",
    "        # Very low confidence - be very conservative\n",
    "        if final_score > 0.6:\n",
    "            return \"WEAK_BUY\"\n",
    "        elif final_score < -0.6:\n",
    "            return \"WEAK_SELL\"\n",
    "        else:\n",
    "            return \"HOLD\"\n",
    "    elif ml_confidence < 0.65:\n",
    "        # Medium confidence - somewhat conservative\n",
    "        if final_score > 0.4:\n",
    "            return \"BUY\"\n",
    "        elif final_score > 0.15:\n",
    "            return \"WEAK_BUY\"\n",
    "        elif final_score < -0.4:\n",
    "            return \"SELL\"\n",
    "        elif final_score < -0.15:\n",
    "            return \"WEAK_SELL\"\n",
    "        else:\n",
    "            return \"HOLD\"\n",
    "    else:\n",
    "        # High confidence - more aggressive recommendations\n",
    "        if final_score > 0.25:\n",
    "            return \"STRONG_BUY\"\n",
    "        elif final_score > 0.1:\n",
    "            return \"BUY\"\n",
    "        elif final_score > 0.05:\n",
    "            return \"WEAK_BUY\"\n",
    "        elif final_score < -0.25:\n",
    "            return \"STRONG_SELL\"\n",
    "        elif final_score < -0.1:\n",
    "            return \"SELL\"\n",
    "        elif final_score < -0.05:\n",
    "            return \"WEAK_SELL\"\n",
    "        else:\n",
    "            return \"HOLD\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
